# 荣耀AI审核发布系统 - 完美方案迭代计划

## 📋 版本规划

### v1.0（当前快速方案）✅
**目标**：快速上线，支持所有内容类型
**实现**：
- ✅ 公众号/美篇：现有方案（精确提取）
- ✅ 其他网站：Crawl4AI（智能提取）
- ✅ 零开发成本，30分钟完成

**优点**：
- 快速上线
- 覆盖所有场景
- 维护成本低

**缺点**：
- 其他网站每次都用AI（如果启用LLM提取）
- 无规则缓存，重复抓取成本高

---

### v2.0（完美方案）🎯
**目标**：成本最优 + 性能最优 + 可维护性最优
**预计工期**：2-3小时开发 + 1周观察调优

#### 核心架构：三层智能抓取

```
┌─────────────────────────────────────────┐
│  第1层：已知网站（精确规则）              │
│  - 公众号/美篇：现有方案                 │
│  - 成本：0元                             │
│  - 速度：最快                            │
└─────────────────────────────────────────┘
              ↓ 未匹配
┌─────────────────────────────────────────┐
│  第2层：AI生成规则 + 缓存                │
│  - 首次：AI分析HTML生成CSS选择器         │
│  - 后续：直接用缓存规则                  │
│  - 成本：首次0.01元，后续0元             │
│  - 速度：快                              │
└─────────────────────────────────────────┘
              ↓ 规则失效/新网站
┌─────────────────────────────────────────┐
│  第3层：Crawl4AI（兜底）                 │
│  - 智能提取，无需规则                    │
│  - 成本：0元（不用LLM）                  │
│  - 速度：中等                            │
└─────────────────────────────────────────┘
```

---

## 🛠️ 实施步骤

### Step 1: 数据库设计（10分钟）

```sql
-- 创建网站抓取规则表
CREATE TABLE IF NOT EXISTS web_extraction_rules (
    id SERIAL PRIMARY KEY,
    domain VARCHAR(255) UNIQUE NOT NULL,
    title_selector VARCHAR(500),
    content_selector VARCHAR(500),
    image_selector VARCHAR(500),
    remove_selectors TEXT[],  -- 需要移除的元素
    success_count INT DEFAULT 0,
    fail_count INT DEFAULT 0,
    last_success_at TIMESTAMP,
    last_fail_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 创建索引
CREATE INDEX idx_domain ON web_extraction_rules(domain);
CREATE INDEX idx_success_rate ON web_extraction_rules(success_count, fail_count);
```

### Step 2: AI规则生成服务（60分钟）

**文件**：`backend/app/services/rule_generator.py`

```python
class RuleGenerator:
    """AI生成CSS选择器规则"""
    
    async def generate_rule(self, url: str, html: str) -> dict:
        """
        分析HTML，生成提取规则
        
        Returns:
            {
                'title_selector': 'h1.article-title',
                'content_selector': 'div.article-content',
                'image_selector': 'div.article-content img',
                'remove_selectors': ['header', 'footer', '.ad']
            }
        """
        prompt = f"""
分析以下HTML结构，生成CSS选择器规则：

URL: {url}
HTML: {html[:5000]}

请返回JSON格式：
{{
    "title_selector": "标题的CSS选择器",
    "content_selector": "正文的CSS选择器",
    "image_selector": "图片的CSS选择器",
    "remove_selectors": ["需要移除的元素选择器"]
}}
"""
        # 调用LLM生成规则
        response = await llm_service.generate(prompt)
        return json.loads(response)
    
    async def validate_rule(self, html: str, rule: dict) -> bool:
        """验证规则是否有效"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # 检查是否能提取到内容
        title = soup.select_one(rule['title_selector'])
        content = soup.select_one(rule['content_selector'])
        
        return title is not None and content is not None
```

### Step 3: 规则缓存服务（30分钟）

**文件**：`backend/app/services/rule_cache.py`

```python
class RuleCache:
    """规则缓存管理"""
    
    async def get_rule(self, domain: str) -> Optional[dict]:
        """获取缓存规则"""
        result = await db.execute(
            "SELECT * FROM web_extraction_rules WHERE domain = $1",
            domain
        )
        return result
    
    async def save_rule(self, domain: str, rule: dict):
        """保存规则"""
        await db.execute("""
            INSERT INTO web_extraction_rules 
            (domain, title_selector, content_selector, image_selector, remove_selectors)
            VALUES ($1, $2, $3, $4, $5)
            ON CONFLICT (domain) DO UPDATE SET
                title_selector = $2,
                content_selector = $3,
                image_selector = $4,
                remove_selectors = $5,
                updated_at = CURRENT_TIMESTAMP
        """, domain, rule['title_selector'], ...)
    
    async def mark_success(self, domain: str):
        """标记规则成功"""
        await db.execute("""
            UPDATE web_extraction_rules 
            SET success_count = success_count + 1,
                last_success_at = CURRENT_TIMESTAMP
            WHERE domain = $1
        """, domain)
    
    async def mark_fail(self, domain: str):
        """标记规则失效"""
        await db.execute("""
            UPDATE web_extraction_rules 
            SET fail_count = fail_count + 1,
                last_fail_at = CURRENT_TIMESTAMP
            WHERE domain = $1
        """, domain)
        
        # 失败3次以上，删除规则
        rule = await self.get_rule(domain)
        if rule['fail_count'] >= 3:
            await db.execute(
                "DELETE FROM web_extraction_rules WHERE domain = $1",
                domain
            )
```

### Step 4: 智能抓取服务（60分钟）

**文件**：`backend/app/services/smart_fetcher.py`

```python
class SmartFetcher:
    """三层智能抓取"""
    
    async def fetch(self, url: str):
        """智能抓取入口"""
        domain = extract_domain(url)
        
        # 第1层：已知网站
        if 'weixin.qq.com' in url:
            return await self._fetch_weixin(url)
        elif 'meipian.cn' in url:
            return await self._fetch_meipian(url)
        
        # 第2层：有缓存规则
        rule = await rule_cache.get_rule(domain)
        if rule:
            try:
                result = await self._fetch_with_rule(url, rule)
                await rule_cache.mark_success(domain)
                return result
            except Exception as e:
                logger.warning(f"规则失效: {domain}, {e}")
                await rule_cache.mark_fail(domain)
        
        # 第3层：生成新规则或用Crawl4AI
        return await self._fetch_with_ai_or_crawl4ai(url, domain)
    
    async def _fetch_with_ai_or_crawl4ai(self, url: str, domain: str):
        """AI生成规则 或 Crawl4AI兜底"""
        
        # 策略A：生成规则（适合重复抓取）
        if await self._should_generate_rule(domain):
            html = await self._get_html(url)
            rule = await rule_generator.generate_rule(url, html)
            
            if await rule_generator.validate_rule(html, rule):
                await rule_cache.save_rule(domain, rule)
                return await self._fetch_with_rule(url, rule)
        
        # 策略B：Crawl4AI（适合一次性抓取）
        return await crawl4ai_fetcher.fetch(url)
    
    async def _should_generate_rule(self, domain: str) -> bool:
        """判断是否值得生成规则"""
        # 检查该域名历史抓取次数
        count = await db.execute(
            "SELECT COUNT(*) FROM submissions WHERE original_url LIKE $1",
            f"%{domain}%"
        )
        # 抓取过2次以上，值得生成规则
        return count >= 2
```

---

## 📊 成本对比

### 场景1：抓取100次公众号文章
```
v1.0: 0元
v2.0: 0元
```

### 场景2：抓取100次新闻网站（10个不同网站，每个10次）
```
v1.0: 
- 不用LLM: 0元
- 用LLM: 100 × 0.01 = 1元

v2.0:
- 首次10次: 10 × 0.01 = 0.1元（生成规则）
- 后续90次: 0元（用缓存规则）
- 总计: 0.1元
```

### 场景3：抓取100次罕见网站（100个不同网站）
```
v1.0: 0元（用Crawl4AI不启用LLM）
v2.0: 0元（直接用Crawl4AI，不生成规则）
```

---

## 🎯 迭代优先级

### 高优先级（必做）
1. ✅ 数据库表创建
2. ✅ AI规则生成服务
3. ✅ 规则缓存服务
4. ✅ 智能抓取服务集成

### 中优先级（建议做）
5. 规则管理界面（查看/编辑/删除规则）
6. 规则失效监控告警
7. 手动调优规则功能

### 低优先级（可选）
8. 规则自动优化（根据成功率调整）
9. 多规则A/B测试
10. 规则共享社区

---

## 📅 时间表

| 阶段 | 任务 | 预计时间 | 负责人 |
|------|------|----------|--------|
| Week 1 | v1.0上线（Crawl4AI集成） | 30分钟 | ✅ 已完成 |
| Week 2 | 数据库设计 + 规则生成服务 | 2小时 | 待定 |
| Week 3 | 规则缓存 + 智能抓取集成 | 2小时 | 待定 |
| Week 4 | 测试 + 观察 + 调优 | 1周 | 待定 |
| Week 5 | 规则管理界面（可选） | 3小时 | 待定 |

---

## ✅ 验收标准

### v2.0上线标准
1. ✅ 公众号/美篇抓取成功率 > 95%
2. ✅ 其他网站首次抓取成功率 > 80%
3. ✅ 缓存规则抓取成功率 > 90%
4. ✅ 平均抓取成本 < 0.005元/次
5. ✅ 规则失效自动切换兜底方案

### 性能指标
- 公众号/美篇：< 3秒
- 有缓存规则：< 5秒
- 无缓存规则（首次）：< 10秒
- Crawl4AI兜底：< 15秒

---

## 🔧 运维监控

### 关键指标
1. 各层抓取成功率
2. 规则缓存命中率
3. 平均抓取成本
4. 规则失效频率

### 告警规则
- 规则失效率 > 10%：发送告警
- 抓取失败率 > 20%：发送告警
- 单日成本 > 10元：发送告警

---

## 📝 备注

- v1.0已完成，可立即使用
- v2.0开发时不影响v1.0运行
- 可根据实际使用情况调整迭代优先级
- 建议先运行v1.0一周，收集数据后再决定是否升级v2.0
