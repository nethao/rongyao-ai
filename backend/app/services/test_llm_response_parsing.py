# -*- coding: utf-8 -*-
"""
æµ‹è¯•LLMå“åº”è§£æåŠŸèƒ½

æµ‹è¯•è¦†ç›–ï¼š
1. å“åº”ç»“æ„éªŒè¯
2. è½¬æ¢å†…å®¹è´¨é‡éªŒè¯
3. å“åº”å…ƒæ•°æ®æå–
4. å„ç§å¼‚å¸¸æƒ…å†µå¤„ç†
"""
import pytest
from unittest.mock import Mock, AsyncMock, patch

from app.services.llm_service import LLMService, LLMTransformError


# æµ‹è¯•ç”¨çš„è™šæ‹ŸAPIå¯†é’¥
TEST_API_KEY = "sk-test-key-for-testing-only"


class TestResponseStructureValidation:
    """æµ‹è¯•å“åº”ç»“æ„éªŒè¯"""
    
    def test_validate_response_structure_valid(self):
        """æµ‹è¯•æœ‰æ•ˆçš„å“åº”ç»“æ„"""
        service = LLMService(api_key=TEST_API_KEY)
        
        # åˆ›å»ºæœ‰æ•ˆçš„å“åº”å¯¹è±¡
        response = Mock()
        response.choices = [Mock()]
        response.choices[0].message = Mock()
        response.choices[0].message.content = "è½¬æ¢åçš„æ–‡æœ¬"
        
        # ä¸åº”è¯¥æŠ›å‡ºå¼‚å¸¸
        service._validate_response_structure(response)
    
    def test_validate_response_structure_none(self):
        """æµ‹è¯•Noneå“åº”"""
        service = LLMService(api_key=TEST_API_KEY)
        
        with pytest.raises(LLMTransformError, match="Response object is None"):
            service._validate_response_structure(None)
    
    def test_validate_response_structure_no_choices(self):
        """æµ‹è¯•ç¼ºå°‘choiceså±æ€§"""
        service = LLMService(api_key=TEST_API_KEY)
        
        response = Mock(spec=[])  # æ²¡æœ‰choiceså±æ€§
        
        with pytest.raises(LLMTransformError, match="missing 'choices' attribute"):
            service._validate_response_structure(response)
    
    def test_validate_response_structure_empty_choices(self):
        """æµ‹è¯•ç©ºçš„choicesåˆ—è¡¨"""
        service = LLMService(api_key=TEST_API_KEY)
        
        response = Mock()
        response.choices = []
        
        with pytest.raises(LLMTransformError, match="has no choices"):
            service._validate_response_structure(response)
    
    def test_validate_response_structure_no_message(self):
        """æµ‹è¯•ç¼ºå°‘messageå±æ€§"""
        service = LLMService(api_key=TEST_API_KEY)
        
        response = Mock()
        response.choices = [Mock(spec=[])]  # æ²¡æœ‰messageå±æ€§
        
        with pytest.raises(LLMTransformError, match="missing 'message' attribute"):
            service._validate_response_structure(response)
    
    def test_validate_response_structure_no_content(self):
        """æµ‹è¯•ç¼ºå°‘contentå±æ€§"""
        service = LLMService(api_key=TEST_API_KEY)
        
        response = Mock()
        response.choices = [Mock()]
        response.choices[0].message = Mock(spec=[])  # æ²¡æœ‰contentå±æ€§
        
        with pytest.raises(LLMTransformError, match="missing 'content' attribute"):
            service._validate_response_structure(response)


class TestTransformedContentValidation:
    """æµ‹è¯•è½¬æ¢å†…å®¹è´¨é‡éªŒè¯"""
    
    def test_validate_transformed_content_valid(self):
        """æµ‹è¯•æœ‰æ•ˆçš„è½¬æ¢å†…å®¹"""
        service = LLMService(api_key=TEST_API_KEY)
        
        original = "è¿™æ˜¯ä¸€æ®µåŸå§‹æ–‡æœ¬ï¼ŒåŒ…å«ä¸€äº›å†…å®¹ã€‚"
        transformed = "è¿™æ˜¯ä¸€æ®µè½¬æ¢åçš„æ–‡æœ¬ï¼ŒåŒ…å«ç›¸ä¼¼çš„å†…å®¹ã€‚"
        
        # ä¸åº”è¯¥æŠ›å‡ºå¼‚å¸¸
        service._validate_transformed_content(original, transformed)
    
    def test_validate_transformed_content_empty(self):
        """æµ‹è¯•ç©ºçš„è½¬æ¢å†…å®¹"""
        service = LLMService(api_key=TEST_API_KEY)
        
        original = "è¿™æ˜¯ä¸€æ®µåŸå§‹æ–‡æœ¬"
        transformed = ""
        
        with pytest.raises(LLMTransformError, match="Transformed content is empty"):
            service._validate_transformed_content(original, transformed)
    
    def test_validate_transformed_content_whitespace_only(self):
        """æµ‹è¯•åªåŒ…å«ç©ºç™½å­—ç¬¦çš„è½¬æ¢å†…å®¹"""
        service = LLMService(api_key=TEST_API_KEY)
        
        original = "è¿™æ˜¯ä¸€æ®µåŸå§‹æ–‡æœ¬"
        transformed = "   \n\t  "
        
        with pytest.raises(LLMTransformError, match="Transformed content is empty"):
            service._validate_transformed_content(original, transformed)
    
    def test_validate_transformed_content_too_short(self):
        """æµ‹è¯•è½¬æ¢å†…å®¹è¿‡çŸ­"""
        service = LLMService(api_key=TEST_API_KEY)
        
        original = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„åŸå§‹æ–‡æœ¬ï¼ŒåŒ…å«å¾ˆå¤šå†…å®¹å’Œç»†èŠ‚ã€‚" * 10
        transformed = "çŸ­æ–‡æœ¬"
        
        with pytest.raises(LLMTransformError, match="too short"):
            service._validate_transformed_content(original, transformed)
    
    def test_validate_transformed_content_too_long_warning(self):
        """æµ‹è¯•è½¬æ¢å†…å®¹è¿‡é•¿ï¼ˆåº”è¯¥åªè®°å½•è­¦å‘Šï¼Œä¸æŠ›å‡ºå¼‚å¸¸ï¼‰"""
        service = LLMService(api_key=TEST_API_KEY)
        
        original = "çŸ­æ–‡æœ¬"
        transformed = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„è½¬æ¢åæ–‡æœ¬ï¼ŒåŒ…å«å¾ˆå¤šæ‰©å±•å†…å®¹å’Œè¯¦ç»†è¯´æ˜ã€‚" * 10
        
        # ä¸åº”è¯¥æŠ›å‡ºå¼‚å¸¸ï¼Œåªè®°å½•è­¦å‘Š
        service._validate_transformed_content(original, transformed)
    
    def test_validate_transformed_content_error_marker_cannot(self):
        """æµ‹è¯•åŒ…å«é”™è¯¯æ ‡è®° 'I cannot'"""
        service = LLMService(api_key=TEST_API_KEY)
        
        original = "è¿™æ˜¯ä¸€æ®µåŸå§‹æ–‡æœ¬"
        transformed = "I cannot process this request because..."
        
        with pytest.raises(LLMTransformError, match="error marker"):
            service._validate_transformed_content(original, transformed)
    
    def test_validate_transformed_content_error_marker_sorry(self):
        """æµ‹è¯•åŒ…å«é”™è¯¯æ ‡è®° 'Sorry'"""
        service = LLMService(api_key=TEST_API_KEY)
        
        original = "è¿™æ˜¯ä¸€æ®µåŸå§‹æ–‡æœ¬"
        transformed = "Sorry, I cannot help with that."
        
        with pytest.raises(LLMTransformError, match="error marker"):
            service._validate_transformed_content(original, transformed)
    
    def test_validate_transformed_content_error_marker_as_ai(self):
        """æµ‹è¯•åŒ…å«é”™è¯¯æ ‡è®° 'As an AI'"""
        service = LLMService(api_key=TEST_API_KEY)
        
        original = "è¿™æ˜¯ä¸€æ®µåŸå§‹æ–‡æœ¬"
        transformed = "As an AI language model, I cannot..."
        
        with pytest.raises(LLMTransformError, match="error marker"):
            service._validate_transformed_content(original, transformed)
    
    def test_validate_transformed_content_case_insensitive(self):
        """æµ‹è¯•é”™è¯¯æ ‡è®°æ£€æµ‹æ˜¯å¤§å°å†™ä¸æ•æ„Ÿçš„"""
        service = LLMService(api_key=TEST_API_KEY)
        
        original = "è¿™æ˜¯ä¸€æ®µåŸå§‹æ–‡æœ¬"
        transformed = "i CaNnOt process this request"
        
        with pytest.raises(LLMTransformError, match="error marker"):
            service._validate_transformed_content(original, transformed)


class TestResponseMetadataExtraction:
    """æµ‹è¯•å“åº”å…ƒæ•°æ®æå–"""
    
    def test_extract_response_metadata_complete(self):
        """æµ‹è¯•æå–å®Œæ•´çš„å…ƒæ•°æ®"""
        service = LLMService(api_key=TEST_API_KEY)
        
        response = Mock()
        response.usage = Mock()
        response.usage.prompt_tokens = 100
        response.usage.completion_tokens = 50
        response.usage.total_tokens = 150
        response.model = "gpt-3.5-turbo"
        response.id = "chatcmpl-123456"
        response.choices = [Mock()]
        response.choices[0].finish_reason = "stop"
        
        metadata = service._extract_response_metadata(response)
        
        assert metadata['prompt_tokens'] == 100
        assert metadata['completion_tokens'] == 50
        assert metadata['total_tokens'] == 150
        assert metadata['model'] == "gpt-3.5-turbo"
        assert metadata['response_id'] == "chatcmpl-123456"
        assert metadata['finish_reason'] == "stop"
    
    def test_extract_response_metadata_partial(self):
        """æµ‹è¯•æå–éƒ¨åˆ†å…ƒæ•°æ®"""
        service = LLMService(api_key=TEST_API_KEY)
        
        # ä½¿ç”¨specé™åˆ¶Mockå¯¹è±¡åªæœ‰ç‰¹å®šå±æ€§
        response = Mock(spec=['model', 'choices'])
        response.model = "gpt-3.5-turbo"
        response.choices = []
        
        metadata = service._extract_response_metadata(response)
        
        assert metadata['model'] == "gpt-3.5-turbo"
        assert 'prompt_tokens' not in metadata
        assert 'finish_reason' not in metadata
    
    def test_extract_response_metadata_empty(self):
        """æµ‹è¯•ä»ç©ºå“åº”æå–å…ƒæ•°æ®"""
        service = LLMService(api_key=TEST_API_KEY)
        
        response = Mock(spec=[])
        
        metadata = service._extract_response_metadata(response)
        
        assert metadata == {}
    
    def test_extract_response_metadata_exception_handling(self):
        """æµ‹è¯•å…ƒæ•°æ®æå–å¼‚å¸¸å¤„ç†"""
        service = LLMService(api_key=TEST_API_KEY)
        
        response = Mock()
        response.usage = Mock()
        # æ¨¡æ‹Ÿè®¿é—®å±æ€§æ—¶æŠ›å‡ºå¼‚å¸¸
        type(response.usage).prompt_tokens = property(lambda self: (_ for _ in ()).throw(Exception("Test error")))
        
        # ä¸åº”è¯¥æŠ›å‡ºå¼‚å¸¸ï¼Œåº”è¯¥è¿”å›ç©ºå­—å…¸
        metadata = service._extract_response_metadata(response)
        
        assert isinstance(metadata, dict)


@pytest.mark.asyncio
class TestTransformTextWithParsing:
    """æµ‹è¯•transform_textæ–¹æ³•çš„å“åº”è§£æé›†æˆ"""
    
    async def test_transform_text_with_valid_response(self):
        """æµ‹è¯•æœ‰æ•ˆå“åº”çš„å®Œæ•´æµç¨‹"""
        service = LLMService(api_key=TEST_API_KEY)
        
        # åˆ›å»ºæ¨¡æ‹Ÿå“åº”
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message = Mock()
        mock_response.choices[0].message.content = "è¿™æ˜¯è½¬æ¢åçš„æ–‡æœ¬å†…å®¹ï¼Œé•¿åº¦é€‚ä¸­ã€‚"
        mock_response.choices[0].finish_reason = "stop"
        mock_response.usage = Mock()
        mock_response.usage.prompt_tokens = 50
        mock_response.usage.completion_tokens = 30
        mock_response.usage.total_tokens = 80
        mock_response.model = "gpt-3.5-turbo"
        mock_response.id = "chatcmpl-test"
        
        with patch.object(service.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value = mock_response
            
            result = await service.transform_text("è¿™æ˜¯åŸå§‹æ–‡æœ¬å†…å®¹ã€‚")
            
            assert result == "è¿™æ˜¯è½¬æ¢åçš„æ–‡æœ¬å†…å®¹ï¼Œé•¿åº¦é€‚ä¸­ã€‚"
            mock_create.assert_called_once()
    
    async def test_transform_text_with_invalid_structure(self):
        """æµ‹è¯•å“åº”ç»“æ„æ— æ•ˆ"""
        service = LLMService(api_key=TEST_API_KEY)
        
        # åˆ›å»ºæ— æ•ˆå“åº”ï¼ˆç¼ºå°‘choicesï¼‰
        mock_response = Mock(spec=[])
        
        with patch.object(service.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value = mock_response
            
            with pytest.raises(LLMTransformError, match="missing 'choices' attribute"):
                await service.transform_text("è¿™æ˜¯åŸå§‹æ–‡æœ¬")
    
    async def test_transform_text_with_empty_content(self):
        """æµ‹è¯•è¿”å›ç©ºå†…å®¹"""
        service = LLMService(api_key=TEST_API_KEY)
        
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message = Mock()
        mock_response.choices[0].message.content = ""
        
        with patch.object(service.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value = mock_response
            
            with pytest.raises(LLMTransformError, match="Transformed content is empty"):
                await service.transform_text("è¿™æ˜¯åŸå§‹æ–‡æœ¬")
    
    async def test_transform_text_with_error_marker(self):
        """æµ‹è¯•è¿”å›åŒ…å«é”™è¯¯æ ‡è®°çš„å†…å®¹"""
        service = LLMService(api_key=TEST_API_KEY)
        
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message = Mock()
        mock_response.choices[0].message.content = "I cannot process this request because it violates policy."
        
        with patch.object(service.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value = mock_response
            
            with pytest.raises(LLMTransformError, match="error marker"):
                await service.transform_text("è¿™æ˜¯åŸå§‹æ–‡æœ¬")
    
    async def test_transform_text_with_too_short_content(self):
        """æµ‹è¯•è¿”å›è¿‡çŸ­çš„å†…å®¹"""
        service = LLMService(api_key=TEST_API_KEY)
        
        original = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„åŸå§‹æ–‡æœ¬ï¼ŒåŒ…å«å¾ˆå¤šå†…å®¹å’Œç»†èŠ‚ã€‚" * 20
        
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message = Mock()
        mock_response.choices[0].message.content = "çŸ­"
        
        with patch.object(service.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value = mock_response
            
            with pytest.raises(LLMTransformError, match="too short"):
                await service.transform_text(original)
    
    async def test_transform_text_empty_input(self):
        """æµ‹è¯•ç©ºè¾“å…¥"""
        service = LLMService(api_key=TEST_API_KEY)
        
        with pytest.raises(ValueError, match="Text content cannot be empty"):
            await service.transform_text("")
    
    async def test_transform_text_whitespace_input(self):
        """æµ‹è¯•åªåŒ…å«ç©ºç™½å­—ç¬¦çš„è¾“å…¥"""
        service = LLMService(api_key=TEST_API_KEY)
        
        with pytest.raises(ValueError, match="Text content cannot be empty"):
            await service.transform_text("   \n\t  ")


@pytest.mark.asyncio
class TestEdgeCases:
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    
    async def test_transform_text_with_exact_min_length(self):
        """æµ‹è¯•è½¬æ¢åå†…å®¹åˆšå¥½è¾¾åˆ°æœ€å°é•¿åº¦"""
        service = LLMService(api_key=TEST_API_KEY)
        
        original = "è¿™æ˜¯åŸå§‹æ–‡æœ¬" * 10  # 60å­—ç¬¦
        transformed = "è½¬æ¢æ–‡æœ¬" * 8  # 32å­—ç¬¦ï¼Œåˆšå¥½è¶…è¿‡50%
        
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message = Mock()
        mock_response.choices[0].message.content = transformed
        mock_response.choices[0].finish_reason = "stop"
        mock_response.usage = Mock()
        mock_response.usage.total_tokens = 50
        
        with patch.object(service.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value = mock_response
            
            result = await service.transform_text(original)
            assert result == transformed
    
    async def test_transform_text_with_unicode_content(self):
        """æµ‹è¯•åŒ…å«Unicodeå­—ç¬¦çš„å†…å®¹"""
        service = LLMService(api_key=TEST_API_KEY)
        
        original = "è¿™æ˜¯åŒ…å«ä¸­æ–‡ã€emoji ğŸ˜€ å’Œç‰¹æ®Šå­—ç¬¦çš„æ–‡æœ¬ Â©Â®â„¢"
        transformed = "è¿™æ˜¯è½¬æ¢ååŒ…å«ä¸­æ–‡ã€emoji ğŸ˜€ å’Œç‰¹æ®Šå­—ç¬¦çš„æ–‡æœ¬ Â©Â®â„¢"
        
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message = Mock()
        mock_response.choices[0].message.content = transformed
        mock_response.choices[0].finish_reason = "stop"
        mock_response.usage = Mock()
        mock_response.usage.total_tokens = 50
        
        with patch.object(service.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value = mock_response
            
            result = await service.transform_text(original)
            assert result == transformed
    
    async def test_transform_text_strips_whitespace(self):
        """æµ‹è¯•è‡ªåŠ¨å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦"""
        service = LLMService(api_key=TEST_API_KEY)
        
        original = "è¿™æ˜¯åŸå§‹æ–‡æœ¬å†…å®¹"
        transformed = "  \nè¿™æ˜¯è½¬æ¢åçš„æ–‡æœ¬å†…å®¹\n  "
        
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message = Mock()
        mock_response.choices[0].message.content = transformed
        mock_response.choices[0].finish_reason = "stop"
        mock_response.usage = Mock()
        mock_response.usage.total_tokens = 50
        
        with patch.object(service.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
            mock_create.return_value = mock_response
            
            result = await service.transform_text(original)
            assert result == "è¿™æ˜¯è½¬æ¢åçš„æ–‡æœ¬å†…å®¹"
            assert not result.startswith(" ")
            assert not result.endswith(" ")
