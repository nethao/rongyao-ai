"""
é‚®ä»¶æŠ“å–ç›¸å…³çš„Celeryä»»åŠ¡
"""
import os
import tempfile
from sqlalchemy import text
from app.tasks import celery_app
from app.database import AsyncSessionLocal
from app.services.imap_fetcher import IMAPFetcher
from app.services.document_processor import DocumentProcessor
from app.services.oss_service import OSSService
from app.services.submission_service import SubmissionService
import logging

logger = logging.getLogger(__name__)


@celery_app.task(name="fetch_emails")
def fetch_emails_task():
    """
    å®šæ—¶æŠ“å–é‚®ä»¶ä»»åŠ¡
    
    è¯¥ä»»åŠ¡ä¼šï¼š
    1. è¿æ¥IMAPæœåŠ¡å™¨
    2. è·å–æœªè¯»é‚®ä»¶
    3. æå–é™„ä»¶å’Œå†…å®¹
    4. è½¬æ¢.docä¸º.docx
    5. æå–å›¾ç‰‡å¹¶ä¸Šä¼ OSS
    6. åˆ›å»ºSubmissionè®°å½•
    7. è§¦å‘AIè½¬æ¢ä»»åŠ¡
    """
    import asyncio
    
    async def _fetch():
        logger.info("å¼€å§‹æ‰§è¡Œé‚®ä»¶æŠ“å–ä»»åŠ¡")
        
        try:
            # åˆå§‹åŒ–æœåŠ¡
            fetcher = IMAPFetcher()
            doc_processor = DocumentProcessor()
            oss_service = OSSService()
            
            # è·å–æœªè¯»é‚®ä»¶
            emails = fetcher.fetch_unread_emails(limit=10, mark_as_read=True)
            logger.info(f"è·å–åˆ° {len(emails)} å°æœªè¯»é‚®ä»¶")
            
            # å¤„ç†æ¯å°é‚®ä»¶
            for email_data in emails:
                try:
                    await process_email(email_data, doc_processor, oss_service)
                except Exception as e:
                    logger.error(f"å¤„ç†é‚®ä»¶å¤±è´¥: {str(e)}")
                    continue
            
            logger.info("é‚®ä»¶æŠ“å–ä»»åŠ¡å®Œæˆ")
            return {"success": True, "processed": len(emails)}
        
        except Exception as e:
            logger.error(f"é‚®ä»¶æŠ“å–ä»»åŠ¡å¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e)}
    
    # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
    return asyncio.run(_fetch())


async def process_email(email_data, doc_processor, oss_service):
    """
    å¤„ç†å•å°é‚®ä»¶
    
    Args:
        email_data: é‚®ä»¶æ•°æ®å¯¹è±¡
        doc_processor: æ–‡æ¡£å¤„ç†å™¨
        oss_service: OSSæœåŠ¡
    """
    from app.services.email_parser import EmailParser, ContentType
    from app.services.web_fetcher import WebFetcher
    
    async with AsyncSessionLocal() as db:
        submission_service = SubmissionService(db)
        
        # è§£æé‚®ä»¶æ ‡é¢˜
        cooperation, media_type, source_unit, title = EmailParser.parse_subject(email_data.subject)
        
        logger.info(f"é‚®ä»¶è§£æç»“æœ - åˆä½œæ–¹å¼:{cooperation}, åª’ä½“:{media_type}, å•ä½:{source_unit}, æ ‡é¢˜:{title}")
        
        # è®°å½•ä»»åŠ¡å¼€å§‹
        await submission_service.log_task(
            task_type="fetch_email",
            task_id=None,
            status="started",
            message=f"å¼€å§‹å¤„ç†é‚®ä»¶: {email_data.subject}"
        )
        
        try:
            # æ£€æµ‹å†…å®¹ç±»å‹
            content_type = EmailParser.detect_content_type(email_data.body, email_data.attachments)
            logger.info(f"å†…å®¹ç±»å‹: {content_type}")
            
            content = email_data.body
            doc_path = None
            docx_path = None
            images_to_upload = []
            original_html = None  # ä¿å­˜åŸå§‹HTML
            
            # æ ¹æ®å†…å®¹ç±»å‹å¤„ç†
            if content_type == ContentType.WEIXIN:
                # æŠ“å–å…¬ä¼—å·æ–‡ç« 
                url = EmailParser.extract_url(email_data.body, ContentType.WEIXIN)
                if url:
                    logger.info(f"æŠ“å–å…¬ä¼—å·æ–‡ç« : {url}")
                    fetcher = WebFetcher()
                    fetched_title, fetched_content, fetched_html, image_urls = fetcher.fetch_weixin_article(url)
                    
                    if fetched_content:
                        content = fetched_content
                        original_html = fetched_html  # ä¿å­˜åŸå§‹HTML
                        # ä¼˜å…ˆä½¿ç”¨æŠ“å–çš„æ ‡é¢˜
                        if fetched_title:
                            title = fetched_title
                        
                        # ä¸‹è½½å›¾ç‰‡
                        for idx, img_url in enumerate(image_urls):
                            img_data = fetcher.download_image(img_url)
                            if img_data:
                                images_to_upload.append((f"weixin_image_{idx}.jpg", img_data))
            
            elif content_type == ContentType.MEIPIAN:
                # æŠ“å–ç¾ç¯‡æ–‡ç« 
                url = EmailParser.extract_url(email_data.body, ContentType.MEIPIAN)
                if url:
                    logger.info(f"æŠ“å–ç¾ç¯‡æ–‡ç« : {url}")
                    fetcher = WebFetcher()
                    fetched_title, fetched_content, image_urls, fetched_html = fetcher.fetch_meipian_article(url)
                    
                    if fetched_content:
                        content = fetched_content
                        original_html = fetched_html  # ä¿å­˜HTMLä¿æŒæ’ç‰ˆ
                        # ä¼˜å…ˆä½¿ç”¨æŠ“å–çš„æ ‡é¢˜
                        if fetched_title:
                            title = fetched_title
                        
                        # ä¸‹è½½å›¾ç‰‡
                        for idx, img_url in enumerate(image_urls):
                            img_data = fetcher.download_image(img_url)
                            if img_data:
                                images_to_upload.append((f"meipian_image_{idx}.jpg", img_data))
            
            elif content_type == ContentType.WORD:
                # å¤„ç†Wordæ–‡æ¡£é™„ä»¶
                for filename, file_data in email_data.attachments:
                    # ä¿å­˜é™„ä»¶åˆ°ä¸´æ—¶æ–‡ä»¶
                    temp_file = tempfile.NamedTemporaryFile(
                        delete=False,
                        suffix=os.path.splitext(filename)[1]
                    )
                    temp_file.write(file_data)
                    temp_file.close()
                    
                    # å¤„ç†Wordæ–‡æ¡£
                    if filename.lower().endswith('.doc'):
                        doc_path = temp_file.name
                        # è½¬æ¢ä¸ºdocx
                        docx_path = doc_processor.convert_doc_to_docx(doc_path)
                        # å…ˆæå–æ ‡é¢˜
                        doc_title, title_lines = doc_processor.extract_title_from_docx(docx_path)
                        if doc_title and doc_title != "æ— æ ‡é¢˜":
                            title = doc_title
                            logger.info(f"ä»Wordæ–‡æ¡£æå–æ ‡é¢˜: {title}, å ç”¨{title_lines}è¡Œ")
                        # æå–æ–‡æœ¬æ—¶è·³è¿‡æ ‡é¢˜è¡Œ
                        content = doc_processor.extract_text_from_docx(docx_path, skip_title_lines=title_lines)
                    
                    elif filename.lower().endswith('.docx'):
                        docx_path = temp_file.name
                        # å…ˆæå–æ ‡é¢˜
                        doc_title, title_lines = doc_processor.extract_title_from_docx(docx_path)
                        if doc_title and doc_title != "æ— æ ‡é¢˜":
                            title = doc_title
                            logger.info(f"ä»Wordæ–‡æ¡£æå–æ ‡é¢˜: {title}, å ç”¨{title_lines}è¡Œ")
                        # æå–æ–‡æœ¬æ—¶è·³è¿‡æ ‡é¢˜è¡Œ
                        content = doc_processor.extract_text_from_docx(docx_path, skip_title_lines=title_lines)
            
            elif content_type == ContentType.ARCHIVE:
                # å¤„ç†å‹ç¼©åŒ… - ç›´æ¥ä¸Šä¼ åˆ°OSS
                archive_urls = []
                for filename, file_data in email_data.attachments:
                    if any(filename.lower().endswith(ext) for ext in ['.zip', '.rar', '.7z']):
                        logger.info(f"å‘ç°å‹ç¼©åŒ…: {filename}, å¤§å°: {len(file_data)/1024/1024:.2f}MB")
                        # ç›´æ¥ä¸Šä¼ åˆ°OSS
                        oss_url, oss_key = oss_service.upload_file(
                            file_data=file_data,
                            filename=filename,
                            folder='archives'
                        )
                        archive_urls.append((filename, oss_url))
                        logger.info(f"å‹ç¼©åŒ…å·²ä¸Šä¼ åˆ°OSS: {oss_url}")
                
                # ç”Ÿæˆä¸‹è½½é“¾æ¥ï¼Œå¹¶ä¿ç•™é‚®ä»¶æ­£æ–‡
                archive_html = "\n\n".join([f'<p><a href="{url}" download="{name}">ğŸ“¦ ä¸‹è½½: {name}</a></p>' for name, url in archive_urls])
                if content:
                    content = f"{content}\n\n{archive_html}"
                else:
                    content = archive_html
            
            elif content_type == ContentType.VIDEO:
                # å¤„ç†è§†é¢‘é™„ä»¶ - ç›´æ¥ä¸Šä¼ åˆ°OSS
                video_urls = []
                for filename, file_data in email_data.attachments:
                    if any(filename.lower().endswith(ext) for ext in ['.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv']):
                        logger.info(f"å‘ç°è§†é¢‘æ–‡ä»¶: {filename}, å¤§å°: {len(file_data)/1024/1024:.2f}MB")
                        # ç›´æ¥ä¸Šä¼ åˆ°OSS
                        oss_url, oss_key = oss_service.upload_file(
                            file_data=file_data,
                            filename=filename,
                            folder='videos'
                        )
                        video_urls.append(oss_url)
                        logger.info(f"è§†é¢‘å·²ä¸Šä¼ åˆ°OSS: {oss_url}")
                
                # ç”Ÿæˆè§†é¢‘åµŒå…¥ä»£ç ï¼Œå¹¶ä¿ç•™é‚®ä»¶æ­£æ–‡
                video_html = "\n\n".join([f'<video controls width="100%"><source src="{url}" type="video/mp4"></video>' for url in video_urls])
                if content:
                    # é‚®ä»¶æ­£æ–‡ + è§†é¢‘
                    content = f"{content}\n\n{video_html}"
                else:
                    content = video_html
            
            # ç¡®å®šå†…å®¹æ¥æº
            if content_type == ContentType.WEIXIN:
                content_source = 'weixin'
            elif content_type == ContentType.MEIPIAN:
                content_source = 'meipian'
            elif content_type == ContentType.ARCHIVE:
                content_source = 'archive'
            elif content_type == ContentType.VIDEO:
                content_source = 'video'
            elif doc_path:
                content_source = 'doc'
            elif docx_path:
                content_source = 'docx'
            else:
                content_source = 'text'
            
            # åˆ›å»ºæŠ•ç¨¿è®°å½•ï¼ˆä½¿ç”¨è§£æåçš„æ ‡é¢˜ï¼Œä¿å­˜åŸå§‹HTMLï¼‰
            submission = await submission_service.create_submission(
                email_subject=title or email_data.subject,
                email_from=email_data.from_addr,
                email_date=email_data.date,
                original_content=content,
                doc_file_path=doc_path,
                docx_file_path=docx_path
            )
            
            # æ›´æ–°content_sourceå’Œoriginal_html
            update_data = {'id': submission.id}
            if original_html:
                update_data['html'] = original_html
                logger.info(f"å·²ä¿å­˜åŸå§‹HTMLï¼Œé•¿åº¦: {len(original_html)}")
            
            await db.execute(
                text('UPDATE submissions SET original_html = :html, content_source = :source WHERE id = :id'),
                {'html': original_html, 'source': content_source, 'id': submission.id}
            )
            await db.commit()
            
            # ä¿å­˜è§£æçš„å…ƒæ•°æ®
            if cooperation or media_type or source_unit:
                site_id = EmailParser.get_wordpress_site_id(media_type) if media_type else None
                
                # æ›´æ–°æŠ•ç¨¿è®°å½•çš„å…ƒæ•°æ®
                await db.execute(
                    text('''
                        UPDATE submissions 
                        SET cooperation_type = :cooperation,
                            media_type = :media,
                            source_unit = :source,
                            target_site_id = :site_id
                        WHERE id = :id
                    '''),
                    {
                        'cooperation': cooperation.value if cooperation else None,
                        'media': media_type.value if media_type else None,
                        'source': source_unit,
                        'site_id': site_id,
                        'id': submission.id
                    }
                )
                await db.commit()
                
                logger.info(f"å…ƒæ•°æ®å·²ä¿å­˜: åˆä½œ={cooperation}, åª’ä½“={media_type}, å•ä½={source_unit}, ç«™ç‚¹={site_id}")
            
            # ä¸Šä¼ ä»ç½‘é¡µæŠ“å–çš„å›¾ç‰‡å¹¶æ›¿æ¢URL
            url_mapping = {}  # åŸå§‹URL -> OSS URLçš„æ˜ å°„ï¼ˆç”¨äº Markdown contentï¼‰
            oss_urls_ordered = []  # æŒ‰å›¾ç‰‡é¡ºåºçš„ OSS URL åˆ—è¡¨ï¼ˆç”¨äº HTML æŒ‰åºæ›¿æ¢ï¼‰
            original_image_urls = []  # ä¿å­˜åŸå§‹å›¾ç‰‡URLé¡ºåº
            
            for idx, (img_filename, img_data) in enumerate(images_to_upload):
                try:
                    oss_url, oss_key = oss_service.upload_file(
                        file_data=img_data,
                        filename=img_filename,
                        folder=f"submissions/{submission.id}"
                    )
                    
                    await submission_service.add_image(
                        submission_id=submission.id,
                        oss_url=oss_url,
                        oss_key=oss_key,
                        original_filename=img_filename,
                        file_size=len(img_data)
                    )
                    
                    oss_urls_ordered.append(oss_url)
                    
                    # è®°å½•åŸå§‹URLï¼ˆä»image_urlsåˆ—è¡¨è·å–ï¼‰
                    if idx < len(image_urls):
                        original_image_urls.append(image_urls[idx])
                    
                    # è®°å½•URLæ˜ å°„ï¼ˆç”¨äºæ›¿æ¢ Markdown contentï¼‰
                    if content_type in [ContentType.WEIXIN, ContentType.MEIPIAN]:
                        import re
                        pattern = rf'!\[å›¾ç‰‡{idx+1}\]\(([^\)]+)\)'
                        match = re.search(pattern, content)
                        if match:
                            original_url = match.group(1)
                            url_mapping[original_url] = oss_url
                            
                except Exception as e:
                    logger.error(f"ä¸Šä¼ å›¾ç‰‡å¤±è´¥: {str(e)}")
                    continue
            
            # æ›¿æ¢ Markdown content ä¸­çš„å›¾ç‰‡URLä¸º OSS URL
            if url_mapping:
                for original_url, oss_url in url_mapping.items():
                    content = content.replace(original_url, oss_url)
            
            # ç¾ç¯‡ï¼šä»HTMLç”Ÿæˆå›¾æ–‡æ··æ’çš„Markdown
            if content_type == ContentType.MEIPIAN and original_html and oss_urls_ordered:
                from bs4 import BeautifulSoup, NavigableString
                soup = BeautifulSoup(original_html, 'html.parser')
                content_tag = soup.find('div', {'class': 'mp-article-tpl'})
                
                if content_tag:
                    markdown_parts = []
                    img_index = 0
                    
                    # éå†æ‰€æœ‰å­å…ƒç´ ï¼Œä¿æŒå›¾æ–‡é¡ºåº
                    for elem in content_tag.descendants:
                        if elem.name == 'img' and img_index < len(oss_urls_ordered):
                            markdown_parts.append(f'\n![å›¾ç‰‡{img_index+1}]({oss_urls_ordered[img_index]})\n')
                            img_index += 1
                        elif isinstance(elem, NavigableString) and elem.strip():
                            elem_text = elem.strip()
                            if elem_text and not elem_text.startswith('[IMAGE_'):
                                markdown_parts.append(elem_text)
                    
                    content = '\n\n'.join([p for p in markdown_parts if p.strip()])
                    logger.info(f"ç¾ç¯‡Markdownå·²ç”Ÿæˆï¼Œä¿æŒå›¾æ–‡æ··æ’ï¼Œ{img_index}å¼ å›¾ç‰‡")
            
            # å…¬ä¼—å·/ç¾ç¯‡ï¼šæ›¿æ¢HTMLä¸­çš„å›¾ç‰‡URLä¸ºOSS URL
            if original_html and content_type in [ContentType.WEIXIN, ContentType.MEIPIAN] and len(oss_urls_ordered) > 0:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(original_html, 'html.parser')
                imgs = soup.find_all('img')
                for i, img in enumerate(imgs):
                    if i < len(oss_urls_ordered):
                        img['src'] = oss_urls_ordered[i]
                        if img.get('data-src'):
                            img['data-src'] = oss_urls_ordered[i]
                original_html = str(soup)
                logger.info(f"HTMLå·²æ›¿æ¢ {len(oss_urls_ordered)} ä¸ªå›¾ç‰‡URLä¸ºOSSåœ°å€")
            
            # æ›´æ–°æŠ•ç¨¿å†…å®¹å’Œ HTML
            if url_mapping or oss_urls_ordered:
                await db.execute(
                    text('UPDATE submissions SET original_content = :content, original_html = :html WHERE id = :id'),
                    {'content': content, 'html': original_html, 'id': submission.id}
                )
                await db.commit()
                logger.info(f"å·²æ›¿æ¢ {len(oss_urls_ordered) or len(url_mapping)} ä¸ªå›¾ç‰‡URLä¸ºOSSåœ°å€")
            
            # æå–å¹¶ä¸Šä¼ Wordæ–‡æ¡£ä¸­çš„å›¾ç‰‡
            if docx_path:
                images = doc_processor.extract_images_from_docx(docx_path)
                
                for img_filename, img_data in images:
                    try:
                        # ä¸Šä¼ åˆ°OSS
                        oss_url, oss_key = oss_service.upload_file(
                            file_data=img_data,
                            filename=img_filename,
                            folder=f"submissions/{submission.id}"
                        )
                        
                        # è®°å½•å›¾ç‰‡ä¿¡æ¯
                        await submission_service.add_image(
                            submission_id=submission.id,
                            oss_url=oss_url,
                            oss_key=oss_key,
                            original_filename=img_filename,
                            file_size=len(img_data)
                        )
                    
                    except Exception as e:
                        logger.error(f"ä¸Šä¼ å›¾ç‰‡å¤±è´¥: {str(e)}")
                        continue
            
            # åˆ›å»ºåŸæ–‡è‰ç¨¿ï¼ˆä¾›ç¼–è¾‘äººå‘˜æŸ¥çœ‹å’Œæ‰‹åŠ¨ç¼–è¾‘ï¼‰
            from app.services.draft_service import DraftService
            from app.utils.content_processor import ContentProcessor
            draft_service = DraftService(db)
            
            # å…¬ä¼—å·å’Œç¾ç¯‡ï¼šå°†HTMLè½¬æ¢ä¸ºMarkdownï¼Œå¹¶æ’å…¥å ä½ç¬¦
            if (content_type == ContentType.WEIXIN or content_type == ContentType.MEIPIAN) and original_html:
                import html2text
                from bs4 import BeautifulSoup
                
                # å…ˆç”¨BeautifulSoupæ›¿æ¢imgæ ‡ç­¾ä¸ºå ä½ç¬¦
                soup = BeautifulSoup(original_html, 'html.parser')
                img_tags = soup.find_all('img')
                
                for idx, img in enumerate(img_tags, start=1):
                    placeholder = f'[[IMG_{idx}]]'
                    img.replace_with(placeholder)
                
                # è½¬æ¢ä¸ºMarkdown
                h = html2text.HTML2Text()
                h.ignore_links = False
                h.ignore_images = False
                h.body_width = 0
                draft_content = h.handle(str(soup))
            else:
                draft_content = content
            
            draft = await draft_service.create_draft(
                submission_id=submission.id,
                transformed_content=draft_content
            )
            logger.info(f"å·²åˆ›å»ºåŸæ–‡è‰ç¨¿: draft_id={draft.id}, content_type={content_type}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºcompleted
            await submission_service.update_status(submission.id, 'completed')
            
            # è®°å½•ä»»åŠ¡æˆåŠŸ
            await submission_service.log_task(
                task_type="fetch_email",
                task_id=str(submission.id),
                status="success",
                message=f"é‚®ä»¶å¤„ç†æˆåŠŸ: submission_id={submission.id}"
            )
            
            logger.info(f"é‚®ä»¶å¤„ç†æˆåŠŸ: submission_id={submission.id}, ç­‰å¾…ç¼–è¾‘äººå‘˜æ“ä½œ")
        
        except Exception as e:
            error_msg = f"é‚®ä»¶å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            # è®°å½•ä»»åŠ¡å¤±è´¥
            await submission_service.log_task(
                task_type="fetch_email",
                task_id=None,
                status="failed",
                message=error_msg
            )
            
            raise


@celery_app.task(name="convert_doc_to_docx")
def convert_doc_to_docx_task(doc_path: str) -> str:
    """
    ä½¿ç”¨LibreOfficeè½¬æ¢æ–‡æ¡£æ ¼å¼
    
    Args:
        doc_path: .docæ–‡ä»¶è·¯å¾„
    
    Returns:
        str: è½¬æ¢åçš„.docxæ–‡ä»¶è·¯å¾„
    """
    try:
        processor = DocumentProcessor()
        docx_path = processor.convert_doc_to_docx(doc_path)
        logger.info(f"æ–‡æ¡£è½¬æ¢æˆåŠŸ: {docx_path}")
        return docx_path
    except Exception as e:
        logger.error(f"æ–‡æ¡£è½¬æ¢å¤±è´¥: {str(e)}")
        raise


@celery_app.task(name="extract_images_from_docx")
def extract_images_task(docx_path: str) -> list:
    """
    ä»docxæå–å›¾ç‰‡å¹¶ä¸Šä¼ OSS
    
    Args:
        docx_path: .docxæ–‡ä»¶è·¯å¾„
    
    Returns:
        list: ä¸Šä¼ åçš„å›¾ç‰‡URLåˆ—è¡¨
    """
    try:
        processor = DocumentProcessor()
        oss_service = OSSService()
        
        # æå–å›¾ç‰‡
        images = processor.extract_images_from_docx(docx_path)
        
        # ä¸Šä¼ åˆ°OSS
        uploaded_urls = []
        for filename, image_data in images:
            oss_url, oss_key = oss_service.upload_file(
                file_data=image_data,
                filename=filename
            )
            uploaded_urls.append(oss_url)
        
        logger.info(f"æˆåŠŸä¸Šä¼  {len(uploaded_urls)} å¼ å›¾ç‰‡")
        return uploaded_urls
    
    except Exception as e:
        logger.error(f"å›¾ç‰‡æå–ä¸Šä¼ å¤±è´¥: {str(e)}")
        raise
