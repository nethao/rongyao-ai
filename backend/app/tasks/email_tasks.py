"""
é‚®ä»¶æŠ“å–ç›¸å…³çš„Celeryä»»åŠ¡
"""
import os
import tempfile  # ä»…ç”¨äºç¡®ä¿ sys.modules['tempfile'] å·²åŠ è½½ï¼Œprocess_email å†…ç”¨ sys.modules å¼•ç”¨
from sqlalchemy import text
from app.tasks import celery_app
from app.database import AsyncSessionLocal
from app.services.imap_fetcher import IMAPFetcher
from app.services.document_processor import DocumentProcessor
from app.services.oss_service import OSSService
from app.services.submission_service import SubmissionService
import logging

logger = logging.getLogger(__name__)


@celery_app.task(name="fetch_emails")
def fetch_emails_task():
    """
    å®šæ—¶æŠ“å–é‚®ä»¶ä»»åŠ¡
    
    è¯¥ä»»åŠ¡ä¼šï¼š
    1. è¿æ¥IMAPæœåŠ¡å™¨
    2. è·å–æœªè¯»é‚®ä»¶
    3. æå–é™„ä»¶å’Œå†…å®¹
    4. è½¬æ¢.docä¸º.docx
    5. æå–å›¾ç‰‡å¹¶ä¸Šä¼ OSS
    6. åˆ›å»ºSubmissionè®°å½•
    7. è§¦å‘AIè½¬æ¢ä»»åŠ¡
    """
    import asyncio
    import nest_asyncio
    
    # å…è®¸åµŒå¥—äº‹ä»¶å¾ªç¯
    nest_asyncio.apply()
    
    async def _fetch():
        logger.info("å¼€å§‹æ‰§è¡Œé‚®ä»¶æŠ“å–ä»»åŠ¡")
        
        # è®°å½•ä»»åŠ¡å¼€å§‹
        async with AsyncSessionLocal() as db:
            from app.services.submission_service import SubmissionService
            service = SubmissionService(db)
            await service.log_task(
                task_type="fetch_email",
                task_id=None,
                status="started",
                message="å¼€å§‹æŠ“å–é‚®ç®±æœªè¯»é‚®ä»¶"
            )
        
        try:
            # åˆå§‹åŒ–æœåŠ¡
            fetcher = IMAPFetcher()
            doc_processor = DocumentProcessor()
            oss_service = OSSService()
            
            # è·å–æœªè¯»é‚®ä»¶
            emails = fetcher.fetch_unread_emails(limit=10, mark_as_read=True, fallback_recent_limit=0)
            logger.info(f"è·å–åˆ° {len(emails)} å°æœªè¯»é‚®ä»¶")
            
            # æ‰¹æ¬¡å†…å»é‡ï¼ˆåŸºäºé‚®ä»¶ä¸»é¢˜ï¼‰
            seen_subjects = set()
            unique_emails = []
            for email_data in emails:
                if email_data.subject not in seen_subjects:
                    seen_subjects.add(email_data.subject)
                    unique_emails.append(email_data)
                else:
                    logger.info(f"æ‰¹æ¬¡å†…é‡å¤é‚®ä»¶ï¼Œè·³è¿‡: {email_data.subject}")
            
            logger.info(f"å»é‡åå‰©ä½™ {len(unique_emails)} å°é‚®ä»¶")
            
            # å¤„ç†æ¯å°é‚®ä»¶
            processed_count = 0
            for email_data in unique_emails:
                try:
                    await process_email(email_data, doc_processor, oss_service)
                    processed_count += 1
                except Exception as e:
                    logger.error(f"å¤„ç†é‚®ä»¶å¤±è´¥: {str(e)}")
                    continue
            
            # è®°å½•ä»»åŠ¡æˆåŠŸ
            async with AsyncSessionLocal() as db:
                from app.services.submission_service import SubmissionService
                service = SubmissionService(db)
                await service.log_task(
                    task_type="fetch_email",
                    task_id=None,
                    status="success",
                    message=f"é‚®ä»¶æŠ“å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} å°é‚®ä»¶"
                )
            
            logger.info("é‚®ä»¶æŠ“å–ä»»åŠ¡å®Œæˆ")
            return {"success": True, "processed": processed_count}
        
        except Exception as e:
            # è®°å½•ä»»åŠ¡å¤±è´¥
            async with AsyncSessionLocal() as db:
                from app.services.submission_service import SubmissionService
                service = SubmissionService(db)
                await service.log_task(
                    task_type="fetch_email",
                    task_id=None,
                    status="failed",
                    message=f"é‚®ä»¶æŠ“å–å¤±è´¥: {str(e)}"
                )
            
            logger.error(f"é‚®ä»¶æŠ“å–ä»»åŠ¡å¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e)}
    
    # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
    return asyncio.run(_fetch())


async def process_email(email_data, doc_processor, oss_service):
    """
    å¤„ç†å•å°é‚®ä»¶
    
    Args:
        email_data: é‚®ä»¶æ•°æ®å¯¹è±¡
        doc_processor: æ–‡æ¡£å¤„ç†å™¨
        oss_service: OSSæœåŠ¡
    """
    from app.services.email_parser import EmailParser, ContentType
    from app.services.web_fetcher import WebFetcher
    import sys
    _tf = sys.modules["tempfile"]

    async with AsyncSessionLocal() as db:
        submission_service = SubmissionService(db)
        
        # è§£æé‚®ä»¶æ ‡é¢˜
        cooperation, media_type, source_unit, title = EmailParser.parse_subject(email_data.subject)
        
        logger.info(f"é‚®ä»¶è§£æç»“æœ - åˆä½œæ–¹å¼:{cooperation}, åª’ä½“:{media_type}, å•ä½:{source_unit}, æ ‡é¢˜:{title}")
        
        # å»é‡åˆ¤æ–­ï¼ˆæ—©åšåˆ¤æ–­ï¼Œé‡å¤åˆ™ä¸æŠ“å–å†…å®¹ï¼‰
        from sqlalchemy import select
        from app.models.submission import Submission
        from app.models.duplicate_log import DuplicateLog
        from app.services.email_parser import CooperationType
        
        # 1) ä¸¥æ ¼ä¸»é¢˜ä¸€è‡´ï¼šæ²¿ç”¨åŸé€»è¾‘
        result = await db.execute(
            select(Submission).where(Submission.email_subject == email_data.subject).limit(1)
        )
        if result.scalar_one_or_none():
            logger.info(f"é‚®ä»¶å·²å¤„ç†è¿‡ï¼ˆä¸»é¢˜ä¸€è‡´ï¼‰ï¼Œè·³è¿‡: {email_data.subject}")
            return
        
        # 2) åŒç¨¿åŒåª’ä½“å»é‡ï¼šéœ€ source_unitã€media_typeã€title å¯è§£æ
        superseded_id = None  # è‹¥å½“å‰é‚®ä»¶èƒœå‡ºå¹¶æ›¿æ¢ï¼Œè®°å½•è¢«æ›¿æ¢çš„æ—§ç¨¿ ID
        if source_unit and media_type and title:
            # æŸ¥è¯¢åŒåª’ä½“ã€åŒå•ä½çš„æŠ•ç¨¿ï¼Œé€ä¸€æ¯”å¯¹æ ‡é¢˜
            r = await db.execute(
                select(Submission).where(
                    Submission.media_type == media_type.value if media_type else Submission.media_type,
                    Submission.source_unit == source_unit
                )
            )
            candidates = []
            for s in r.scalars().all():
                existing_title = EmailParser.extract_title_for_dedup(s.email_subject or "")
                if existing_title and existing_title.strip() == title.strip():
                    candidates.append(s)
            
            if candidates:
                def _dedup_score(sub):
                    """ä¼˜å…ˆçº§ï¼šåˆä½œ>æŠ•ç¨¿ï¼Œæ–°ç¨¿>æ—§ç¨¿ã€‚åˆ†æ•°è¶Šå°è¶Šä¼˜"""
                    coop_rank = 0 if sub.cooperation_type == "partner" else 1
                    ts = sub.email_date.timestamp() if sub.email_date else 0
                    return (coop_rank, -ts)
                
                best = min(candidates, key=_dedup_score)
                curr_rank = 0 if cooperation == CooperationType.PARTNER else 1
                curr_ts = email_data.date.timestamp() if email_data.date else 0
                curr_score = (curr_rank, -curr_ts)
                best_score = _dedup_score(best)
                
                if curr_score < best_score:
                    # å½“å‰é‚®ä»¶èƒœå‡ºï¼Œå°†å¤„ç†å¹¶æ›¿æ¢æ—§ç¨¿
                    superseded_id = best.id
                    logger.info(f"åŒç¨¿åŒåª’ä½“ï¼Œå½“å‰é‚®ä»¶ä¼˜äºå·²æœ‰ç¨¿ï¼Œå°†æ›¿æ¢: æ—§ç¨¿ID={best.id}, ä¸»é¢˜={email_data.subject}")
                else:
                    # å·²æœ‰ç¨¿èƒœå‡ºï¼Œè·³è¿‡å½“å‰é‚®ä»¶ï¼Œä¸æŠ“å–å†…å®¹
                    dup_log = DuplicateLog(
                        email_subject=email_data.subject,
                        email_from=email_data.from_addr,
                        email_date=email_data.date,
                        cooperation_type=cooperation.value if cooperation else None,
                        media_type=media_type.value if media_type else None,
                        source_unit=source_unit,
                        title=title,
                        duplicate_type="skipped",
                        effective_submission_id=best.id,
                    )
                    db.add(dup_log)
                    await db.commit()
                    logger.info(f"é‡å¤ç¨¿ä»¶å·²è·³è¿‡ï¼ˆä¸æŠ“å–å†…å®¹ï¼‰: {email_data.subject} -> æœ‰æ•ˆç¨¿ID={best.id}")
                    return
        
        # è®°å½•ä»»åŠ¡å¼€å§‹
        await submission_service.log_task(
            task_type="fetch_email",
            task_id=None,
            status="started",
            message=f"å¼€å§‹å¤„ç†é‚®ä»¶: {email_data.subject}"
        )
        
        try:
            # æ£€æµ‹å†…å®¹ç±»å‹
            content_type = EmailParser.detect_content_type(email_data.body, email_data.attachments)
            logger.info(f"å†…å®¹ç±»å‹: {content_type}")
            
            content = email_data.body
            doc_path = None
            docx_path = None
            images_to_upload = []
            image_urls = []
            original_html = None  # ä¿å­˜åŸå§‹HTML
            attachment_records = []  # å¾…å†™å…¥çš„é™„ä»¶è®°å½•ï¼ˆOSSå·²ä¸Šä¼ ï¼‰
            
            # æ ¹æ®å†…å®¹ç±»å‹å¤„ç†
            if content_type == ContentType.WEIXIN:
                # æŠ“å–å…¬ä¼—å·æ–‡ç« 
                url = EmailParser.extract_url(email_data.body, ContentType.WEIXIN)
                if url:
                    logger.info(f"æŠ“å–å…¬ä¼—å·æ–‡ç« : {url}")
                    fetcher = WebFetcher()
                    fetched_title, fetched_content, fetched_html, image_urls = fetcher.fetch_weixin_article(url)
                    
                    if fetched_content:
                        content = fetched_content
                        original_html = fetched_html  # ä¿å­˜åŸå§‹HTML
                        # ä¼˜å…ˆä½¿ç”¨æŠ“å–çš„æ ‡é¢˜
                        if fetched_title:
                            title = fetched_title
                        
                        # ä¸‹è½½å›¾ç‰‡
                        for idx, img_url in enumerate(image_urls):
                            img_data = fetcher.download_image(img_url)
                            if img_data:
                                images_to_upload.append((f"weixin_image_{idx}.jpg", img_data))
            
            elif content_type == ContentType.MEIPIAN:
                # æŠ“å–ç¾ç¯‡æ–‡ç« 
                url = EmailParser.extract_url(email_data.body, ContentType.MEIPIAN)
                if url:
                    logger.info(f"æŠ“å–ç¾ç¯‡æ–‡ç« : {url}")
                    fetcher = WebFetcher()
                    fetched_title, fetched_content, image_urls, fetched_html = fetcher.fetch_meipian_article(url)
                    
                    if fetched_content:
                        content = fetched_content
                        original_html = fetched_html  # ä¿å­˜HTMLä¿æŒæ’ç‰ˆ
                        # ä¼˜å…ˆä½¿ç”¨æŠ“å–çš„æ ‡é¢˜
                        if fetched_title:
                            title = fetched_title
                        
                        # ä¸‹è½½å›¾ç‰‡
                        for idx, img_url in enumerate(image_urls):
                            img_data = fetcher.download_image(img_url)
                            if img_data:
                                images_to_upload.append((f"meipian_image_{idx}.jpg", img_data))
            
            elif content_type == ContentType.LARGE_ATTACHMENT:
                # è¶…å¤§é™„ä»¶ï¼šæå–æ‰€æœ‰ä¸‹è½½é“¾æ¥ï¼Œç”±ç¼–è¾‘äººå‘˜æ‰‹åŠ¨ä¸‹è½½
                import re
                import html as html_module
                
                # æå–æ‰€æœ‰ QQ é‚®ç®±å’Œç½‘æ˜“é‚®ç®±çš„è¶…å¤§é™„ä»¶ä¸‹è½½é“¾æ¥
                qq_links = re.findall(r'https://wx\.mail\.qq\.com/ftn/download[^\s<>"\']+', email_data.body)
                netease_links = re.findall(r'https://mail\.163\.com/large-attachment-download/[^\s<>"\']+', email_data.body)
                
                # HTML è§£ç é“¾æ¥ï¼ˆ&amp; -> &ï¼‰
                qq_links = [html_module.unescape(link) for link in qq_links]
                netease_links = [html_module.unescape(link) for link in netease_links]
                
                # å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
                seen = set()
                all_links = []
                for link in qq_links + netease_links:
                    if link not in seen:
                        seen.add(link)
                        all_links.append(link)
                
                if not all_links:
                    logger.warning("æœªæ‰¾åˆ°è¶…å¤§é™„ä»¶ä¸‹è½½é“¾æ¥")
                    content = email_data.body or ""
                    original_html = f'<html><body><pre>{content}</pre></body></html>'
                else:
                    logger.info(f"æ£€æµ‹åˆ° {len(all_links)} ä¸ªè¶…å¤§é™„ä»¶ä¸‹è½½é“¾æ¥")
                    
                    # å°è¯•æå–æ–‡ä»¶å
                    filenames = []
                    for link in all_links:
                        # ä» title æˆ–é“¾æ¥æ–‡æœ¬æå–æ–‡ä»¶å
                        title_match = re.search(rf'title="([^"]+)"[^>]*>{re.escape(link)}', email_data.body)
                        if title_match:
                            filenames.append(title_match.group(1).split('\n')[0].strip())
                        else:
                            # å°è¯•ä» URL å‚æ•°æå–
                            title_param = re.search(r'[?&]title=([^&]+)', link)
                            if title_param:
                                import urllib.parse
                                filenames.append(urllib.parse.unquote(title_param.group(1)))
                            else:
                                filenames.append(f"é™„ä»¶ {len(filenames) + 1}")
                    
                    body_text = (email_data.body or "").strip()
                    
                    # ç”Ÿæˆå†…å®¹æ‘˜è¦
                    content_lines = [f"è¶…å¤§é™„ä»¶ ({len(all_links)} ä¸ª)"]
                    for i, (link, filename) in enumerate(zip(all_links, filenames), 1):
                        content_lines.append(f"{i}. {filename}: {link}")
                    content_lines.append(f"\n{body_text}")
                    content = "\n".join(content_lines)
                    
                    # ç”ŸæˆåŸå§‹å†…å®¹é¢„è§ˆ HTML
                    download_items = []
                    for i, (link, filename) in enumerate(zip(all_links, filenames), 1):
                        download_items.append(
                            f'<div class="download-item">'
                            f'<p class="filename">ğŸ“ {i}. {filename}</p>'
                            f'<p><a href="{link}" target="_blank">ç‚¹å‡»æ­¤å¤„è¿›å…¥ä¸‹è½½é¡µé¢</a></p>'
                            f'</div>'
                        )
                    
                    original_html = (
                        '<html><head><meta charset="utf-8">'
                        '<style>body{font-family:sans-serif;padding:20px;} '
                        'a{color:#409eff;font-size:16px;word-break:break-all;} '
                        '.download-section{background:#fff3cd;padding:15px;border-left:4px solid #ffc107;margin:10px 0;} '
                        '.download-item{margin:10px 0;padding:10px;background:#fff;border-radius:4px;} '
                        '.download-item a{color:#e6a23c;font-weight:bold;font-size:16px;} '
                        '.filename{color:#303133;font-size:14px;margin:5px 0;} '
                        'pre{white-space:pre-wrap;word-break:break-all;color:#606266;font-size:13px;}</style></head>'
                        '<body>'
                        '<div class="download-section">'
                        f'<p><strong>âš ï¸ è¶…å¤§é™„ä»¶ ({len(all_links)} ä¸ªï¼Œè¯·ç‚¹å‡»ä¸‹è½½æŒ‰é’®æ‰‹åŠ¨ä¸‹è½½ï¼‰ï¼š</strong></p>'
                        + ''.join(download_items) +
                        '</div>'
                        f'<hr><p><strong>é‚®ä»¶åŸæ–‡ï¼š</strong></p><pre>{body_text}</pre>'
                        '</body></html>'
                    )
            
            elif content_type == ContentType.OTHER_URL:
                # äººå·¥é‡‡é›†æ¨¡å¼ï¼šåªä¿å­˜é“¾æ¥å’Œé‚®ä»¶åŸæ–‡ï¼Œä¸è‡ªåŠ¨æŠ“å–ç½‘é¡µå†…å®¹
                url = EmailParser.extract_url(email_data.body, ContentType.OTHER_URL)
                url_line = f"é“¾æ¥: {url}" if url else ""
                body_text = (email_data.body or "").strip()
                content = "\n\n".join(filter(None, [url_line, body_text])) or url_line or ""

                # ç”ŸæˆåŸå§‹å†…å®¹é¢„è§ˆ HTMLï¼ˆä¾›å®¡æ ¸é¡µå·¦æ å±•ç¤ºå¯ç‚¹å‡»é“¾æ¥ï¼‰
                if url:
                    original_html = (
                        '<html><head><meta charset="utf-8">'
                        '<style>body{font-family:sans-serif;padding:20px;} '
                        'a{color:#409eff;font-size:16px;word-break:break-all;} '
                        'pre{white-space:pre-wrap;word-break:break-all;color:#606266;font-size:13px;}</style></head>'
                        f'<body><p><strong>å¤–éƒ¨é“¾æ¥ï¼ˆè¯·ç‚¹å‡»æ‰“å¼€åæ‰‹åŠ¨å¤åˆ¶å†…å®¹ï¼‰ï¼š</strong></p>'
                        f'<p><a href="{url}" target="_blank">{url}</a></p>'
                        f'<hr><p><strong>é‚®ä»¶åŸæ–‡ï¼š</strong></p><pre>{body_text}</pre></body></html>'
                    )
            
            # Word æ–‡æ¡£å¤„ç†ï¼ˆåŒ…æ‹¬è¶…å¤§é™„ä»¶ä¸‹è½½æˆåŠŸåè½¬æ¢çš„ï¼‰
            if content_type == ContentType.WORD:
                # å¤„ç†Wordæ–‡æ¡£é™„ä»¶
                logger.info("å¼€å§‹å¤„ç†Wordæ–‡æ¡£")
                temp_file_path = None
                docx_path_to_clean = None
                word_attachment_images = []
                
                try:
                    logger.info(f"Wordé™„ä»¶æ•°é‡: {len(email_data.attachments)}")
                    for filename, file_data in email_data.attachments:
                        filename_lower = filename.lower()
                        logger.info(f"å¤„ç†Wordé™„ä»¶: {filename}")

                        # é‚®ä»¶é‡Œ"Word + ç‹¬ç«‹å›¾ç‰‡é™„ä»¶"åœºæ™¯ï¼šå›¾ç‰‡ä¸åœ¨ docx å†…ï¼Œéœ€ä¸€å¹¶å…¥åº“å¹¶æ’å…¥å ä½ç¬¦
                        if any(filename_lower.endswith(ext) for ext in [".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp"]):
                            word_attachment_images.append((filename, file_data))
                            logger.info(f"è¯†åˆ«åˆ°ç‹¬ç«‹å›¾ç‰‡é™„ä»¶: {filename}")
                            continue

                        # é Word é™„ä»¶åœ¨è¯¥åˆ†æ”¯è·³è¿‡
                        if not filename_lower.endswith(('.doc', '.docx')):
                            logger.info(f"è·³è¿‡éWordé™„ä»¶: {filename}")
                            continue

                        # ä¸Šä¼ åŸå§‹Wordé™„ä»¶åˆ°OSSï¼Œè®°å½•é™„ä»¶
                        try:
                            oss_url, oss_key = oss_service.upload_file(
                                file_data=file_data,
                                filename=filename,
                                folder='attachments'
                            )
                            attachment_records.append({
                                "attachment_type": "word",
                                "oss_url": oss_url,
                                "oss_key": oss_key,
                                "original_filename": filename,
                                "file_size": len(file_data) if file_data else None
                            })
                        except Exception as e:
                            logger.error(f"Wordé™„ä»¶ä¸Šä¼ å¤±è´¥: {filename}, err={e}")

                        # ä¿å­˜ Word é™„ä»¶åˆ°ä¸´æ—¶æ–‡ä»¶
                        temp_file = _tf.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1])
                        temp_file.write(file_data)
                        temp_file.close()
                        temp_file_path = temp_file.name
                        logger.info(f"Wordä¸´æ—¶æ–‡ä»¶å·²ä¿å­˜: {temp_file_path}")
                        
                        # å¤„ç†Wordæ–‡æ¡£
                        if filename_lower.endswith('.doc'):
                            doc_path = temp_file_path
                            # è½¬æ¢ä¸ºdocxï¼ˆå¯èƒ½è¾ƒæ…¢ï¼Œæœ€å¤šçº¦ 120 ç§’ï¼‰
                            logger.info("å¼€å§‹å°† .doc è½¬ä¸º .docx")
                            docx_path = doc_processor.convert_doc_to_docx(doc_path)
                            docx_path_to_clean = docx_path
                            logger.info(".doc è½¬æ¢å®Œæˆï¼Œå¼€å§‹æå–æ ‡é¢˜ä¸æ­£æ–‡")
                            # å…ˆæå–æ ‡é¢˜
                            doc_title, title_lines = doc_processor.extract_title_from_docx(docx_path)
                            if doc_title and doc_title != "æ— æ ‡é¢˜":
                                title = doc_title
                                logger.info(f"ä»Wordæ–‡æ¡£æå–æ ‡é¢˜: {title}, å ç”¨{title_lines}è¡Œ")
                            # æå–æ–‡æœ¬æ—¶è·³è¿‡æ ‡é¢˜è¡Œ
                            content = doc_processor.extract_text_from_docx(docx_path, skip_title_lines=title_lines)
                        
                        elif filename_lower.endswith('.docx'):
                            docx_path = temp_file_path
                            logger.info("å¼€å§‹ä» .docx æå–æ ‡é¢˜ä¸æ­£æ–‡")
                            # å…ˆæå–æ ‡é¢˜
                            doc_title, title_lines = doc_processor.extract_title_from_docx(docx_path)
                            if doc_title and doc_title != "æ— æ ‡é¢˜":
                                title = doc_title
                                logger.info(f"ä»Wordæ–‡æ¡£æå–æ ‡é¢˜: {title}, å ç”¨{title_lines}è¡Œ")
                            # æå–æ–‡æœ¬æ—¶è·³è¿‡æ ‡é¢˜è¡Œ
                            content = doc_processor.extract_text_from_docx(docx_path, skip_title_lines=title_lines)

                    # æå– Word å†…åµŒå›¾ç‰‡ï¼ˆä¿æŒ [[IMG_1]] èµ·å§‹é¡ºåºï¼‰
                    if docx_path and os.path.exists(docx_path):
                        embedded_images = doc_processor.extract_images_from_docx(docx_path)
                        logger.info(f"ä»Wordæ–‡æ¡£æå–å†…åµŒå›¾ç‰‡ {len(embedded_images)} å¼ ")
                        images_to_upload.extend(embedded_images)

                    # å°†"ç‹¬ç«‹å›¾ç‰‡é™„ä»¶"è¿½åŠ åˆ°å†…å®¹æœ«å°¾ï¼Œå ä½ç¬¦ä»ç°æœ‰æœ€å¤§åºå·ç»§ç»­
                    if word_attachment_images:
                        import re
                        existing_indexes = [
                            int(x) for x in re.findall(r"\[\[IMG_(\d+)\]\]", content or "")
                        ]
                        next_idx = (max(existing_indexes) if existing_indexes else 0) + 1
                        for filename, file_data in word_attachment_images:
                            content = f"{content}\n\n[[IMG_{next_idx}]]"
                            images_to_upload.append((filename, file_data))
                            next_idx += 1
                        logger.info(f"å·²è¿½åŠ ç‹¬ç«‹å›¾ç‰‡é™„ä»¶ {len(word_attachment_images)} å¼ å¹¶å†™å…¥å ä½ç¬¦")

                    # å¤„ç†è§†é¢‘é™„ä»¶ï¼ˆæ–‡æ¡£+è§†é¢‘åœºæ™¯ï¼‰
                    video_exts = {'.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv'}
                    for filename, file_data in email_data.attachments:
                        if any(filename.lower().endswith(ext) for ext in video_exts):
                            logger.info(f"å‘ç°è§†é¢‘é™„ä»¶: {filename}, å¤§å°: {len(file_data)/1024/1024:.2f}MB")
                            oss_url, oss_key = oss_service.upload_file(
                                file_data=file_data,
                                filename=filename,
                                folder='videos'
                            )
                            attachment_records.append({
                                "attachment_type": "video",
                                "oss_url": oss_url,
                                "oss_key": oss_key,
                                "original_filename": filename,
                                "file_size": len(file_data) if file_data else None
                            })
                            video_tag = f'<video controls width="100%"><source src="{oss_url}" type="video/mp4"></video>'
                            content = f"{content}\n\n{video_tag}"
                            logger.info(f"è§†é¢‘å·²ä¸Šä¼ OSSå¹¶è¿½åŠ åˆ°å†…å®¹: {oss_url}")

                except Exception as e:
                    logger.error(f"Wordå¤„ç†å¼‚å¸¸: {e}", exc_info=True)
                    raise
                finally:
                    # ä¸´æ—¶æ–‡ä»¶æ¸…ç†åç§»åˆ°"Wordå›¾ç‰‡æå–å¹¶ä¸Šä¼ "ä¹‹åï¼Œ
                    # å¦åˆ™ä¼šå¯¼è‡´ docx_path ä¸å­˜åœ¨ï¼Œå›¾ç‰‡æ— æ³•æå–ï¼Œåªå‰©å ä½ç¬¦ã€‚
                    logger.info(
                        f"æš‚ç¼“æ¸…ç†Wordä¸´æ—¶æ–‡ä»¶: temp_file_path={temp_file_path}, "
                        f"docx_path_to_clean={docx_path_to_clean}"
                    )
            
            elif content_type == ContentType.ARCHIVE:
                # å¤„ç†å‹ç¼©åŒ… - è§£å‹å¹¶å¤„ç† Word + å›¾ç‰‡ + è§†é¢‘
                import zipfile
                import shutil
                import subprocess

                _video_exts = {'.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv'}

                def _extract_archive(file_path: str, filename: str, extract_dir: str):
                    fname_lower = filename.lower()
                    if fname_lower.endswith('.zip'):
                        with zipfile.ZipFile(file_path, 'r') as zip_ref:
                            for member in zip_ref.infolist():
                                target = os.path.realpath(
                                    os.path.join(extract_dir, member.filename)
                                )
                                if not target.startswith(
                                    os.path.realpath(extract_dir) + os.sep
                                ) and target != os.path.realpath(extract_dir):
                                    raise ValueError(f"Zip Slip æ£€æµ‹åˆ°æ¶æ„è·¯å¾„: {member.filename}")
                            zip_ref.extractall(extract_dir)
                        return

                    # rar/7z: ä¼˜å…ˆ 7zï¼Œå…¶æ¬¡ unrar
                    if shutil.which('7z'):
                        result = subprocess.run(
                            ['7z', 'x', '-y', f'-o{extract_dir}', file_path],
                            capture_output=True,
                            text=True
                        )
                        if result.returncode != 0:
                            raise RuntimeError(result.stderr or result.stdout or '7z è§£å‹å¤±è´¥')
                        return
                    if shutil.which('unrar'):
                        result = subprocess.run(
                            ['unrar', 'x', '-o+', file_path, extract_dir],
                            capture_output=True,
                            text=True
                        )
                        if result.returncode != 0:
                            raise RuntimeError(result.stderr or result.stdout or 'unrar è§£å‹å¤±è´¥')
                        return

                    raise RuntimeError('RAR/7Z è§£å‹ä¾èµ–æœªå®‰è£…ï¼ˆéœ€ 7z æˆ– unrarï¼‰')

                for filename, file_data in email_data.attachments:
                    fname_lower = filename.lower()

                    if not fname_lower.endswith(('.zip', '.rar', '.7z')):
                        continue

                    logger.info(f"å‘ç°å‹ç¼©åŒ…: {filename}, å¤§å°: {len(file_data)/1024/1024:.2f}MB")

                    # ä¸Šä¼ åŸå§‹å‹ç¼©åŒ…åˆ°OSSï¼Œè®°å½•é™„ä»¶
                    try:
                        arc_url, arc_key = oss_service.upload_file(
                            file_data=file_data,
                            filename=filename,
                            folder='attachments'
                        )
                        attachment_records.append({
                            "attachment_type": "archive",
                            "oss_url": arc_url,
                            "oss_key": arc_key,
                            "original_filename": filename,
                            "file_size": len(file_data) if file_data else None
                        })
                    except Exception as e:
                        logger.error(f"å‹ç¼©åŒ…ä¸Šä¼ å¤±è´¥: {filename}, err={e}")

                    with _tf.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as zip_file:
                        zip_file.write(file_data)
                        zip_path = zip_file.name

                    extract_dir = _tf.mkdtemp()
                    try:
                        _extract_archive(zip_path, filename, extract_dir)
                        logger.info(f"å‹ç¼©åŒ…å·²è§£å‹åˆ°: {extract_dir}")

                        # â”€â”€ æŸ¥æ‰¾å¹¶å¤„ç† Word æ–‡æ¡£ â”€â”€
                        word_file = None
                        for root, dirs, files in os.walk(extract_dir):
                            for file in files:
                                if file.lower().endswith(('.doc', '.docx')):
                                    word_file = os.path.join(root, file)
                                    break
                            if word_file:
                                break

                        if word_file:
                            logger.info(f"æ‰¾åˆ°Wordæ–‡æ¡£: {word_file}")
                            try:
                                with open(word_file, 'rb') as wf:
                                    word_data = wf.read()
                                w_url, w_key = oss_service.upload_file(
                                    file_data=word_data,
                                    filename=os.path.basename(word_file),
                                    folder='attachments'
                                )
                                attachment_records.append({
                                    "attachment_type": "word",
                                    "oss_url": w_url,
                                    "oss_key": w_key,
                                    "original_filename": os.path.basename(word_file),
                                    "file_size": len(word_data) if word_data else None
                                })
                            except Exception as e:
                                logger.error(f"å‹ç¼©åŒ…å†…Wordä¸Šä¼ å¤±è´¥: {word_file}, err={e}")
                            if word_file.lower().endswith('.doc'):
                                docx_path = doc_processor.convert_doc_to_docx(word_file)
                            else:
                                docx_path = word_file

                            doc_title, title_lines = doc_processor.extract_title_from_docx(docx_path)
                            if doc_title and doc_title != "æ— æ ‡é¢˜":
                                title = doc_title
                                logger.info(f"ä»Wordæ–‡æ¡£æå–æ ‡é¢˜: {title}, å ç”¨{title_lines}è¡Œ")

                            content = doc_processor.extract_text_from_docx(docx_path, skip_title_lines=title_lines)

                            embedded_images = doc_processor.extract_images_from_docx(docx_path)
                            logger.info(f"ä»Wordæ–‡æ¡£æå–{len(embedded_images)}å¼ å›¾ç‰‡")
                            for img_filename, img_data in embedded_images:
                                images_to_upload.append((img_filename, img_data))
                        else:
                            logger.warning("å‹ç¼©åŒ…ä¸­æœªæ‰¾åˆ°Wordæ–‡æ¡£")
                            content = "å‹ç¼©åŒ…ä¸­æœªæ‰¾åˆ°Wordæ–‡æ¡£"

                        # â”€â”€ æŸ¥æ‰¾å¹¶å¤„ç†ç‹¬ç«‹å›¾ç‰‡æ–‡ä»¶ â”€â”€
                        image_exts = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}
                        standalone_images = []
                        for root, dirs, files in os.walk(extract_dir):
                            for file in files:
                                file_ext = '.' + file.rsplit('.', 1)[-1].lower() if '.' in file else ''
                                if file_ext in image_exts:
                                    img_path = os.path.join(root, file)
                                    logger.info(f"å‹ç¼©åŒ…å†…å‘ç°å›¾ç‰‡: {file}")
                                    with open(img_path, 'rb') as img_f:
                                        img_data = img_f.read()
                                    standalone_images.append((file, img_data))
                        
                        # å°†ç‹¬ç«‹å›¾ç‰‡æ·»åŠ åˆ°å†…å®¹æœ«å°¾ï¼ˆä½¿ç”¨å ä½ç¬¦ï¼‰
                        if standalone_images:
                            # æ‰¾åˆ°ç°æœ‰å ä½ç¬¦çš„æœ€å¤§åºå·
                            import re
                            existing_indexes = [
                                int(x) for x in re.findall(r"\[\[IMG_(\d+)\]\]", content or "")
                            ]
                            next_idx = (max(existing_indexes) if existing_indexes else 0) + 1
                            
                            for img_filename, img_data in standalone_images:
                                images_to_upload.append((img_filename, img_data))
                                # åœ¨å†…å®¹ä¸­æ·»åŠ å ä½ç¬¦
                                placeholder = f"[[IMG_{next_idx}]]"
                                content = f"{content}\n\n{placeholder}"
                                next_idx += 1
                            logger.info(f"å‹ç¼©åŒ…å†…æ·»åŠ {len(standalone_images)}å¼ ç‹¬ç«‹å›¾ç‰‡åˆ°å†…å®¹")

                        # â”€â”€ æŸ¥æ‰¾å¹¶å¤„ç†è§†é¢‘æ–‡ä»¶ â”€â”€
                        for root, dirs, files in os.walk(extract_dir):
                            for file in files:
                                file_ext = '.' + file.rsplit('.', 1)[-1].lower() if '.' in file else ''
                                if file_ext in _video_exts:
                                    video_path = os.path.join(root, file)
                                    logger.info(f"å‹ç¼©åŒ…å†…å‘ç°è§†é¢‘: {file}")
                                    with open(video_path, 'rb') as vf:
                                        video_data = vf.read()
                                    oss_url, oss_key = oss_service.upload_file(
                                        file_data=video_data,
                                        filename=file,
                                        folder='videos'
                                    )
                                    attachment_records.append({
                                        "attachment_type": "video",
                                        "oss_url": oss_url,
                                        "oss_key": oss_key,
                                        "original_filename": file,
                                        "file_size": len(video_data) if video_data else None
                                    })
                                    video_tag = f'<video controls width="100%"><source src="{oss_url}" type="video/mp4"></video>'
                                    content = f"{content}\n\n{video_tag}"
                                    logger.info(f"å‹ç¼©åŒ…å†…è§†é¢‘å·²ä¸Šä¼ OSS: {oss_url}")

                    finally:
                        shutil.rmtree(extract_dir, ignore_errors=True)
                        os.unlink(zip_path)
            
            elif content_type == ContentType.VIDEO:
                # å¤„ç†è§†é¢‘é™„ä»¶ - ç›´æ¥ä¸Šä¼ åˆ°OSS
                video_urls = []
                for filename, file_data in email_data.attachments:
                    if any(filename.lower().endswith(ext) for ext in ['.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv']):
                        logger.info(f"å‘ç°è§†é¢‘æ–‡ä»¶: {filename}, å¤§å°: {len(file_data)/1024/1024:.2f}MB")
                        # ç›´æ¥ä¸Šä¼ åˆ°OSS
                        oss_url, oss_key = oss_service.upload_file(
                            file_data=file_data,
                            filename=filename,
                            folder='videos'
                        )
                        attachment_records.append({
                            "attachment_type": "video",
                            "oss_url": oss_url,
                            "oss_key": oss_key,
                            "original_filename": filename,
                            "file_size": len(file_data) if file_data else None
                        })
                        video_urls.append(oss_url)
                        logger.info(f"è§†é¢‘å·²ä¸Šä¼ åˆ°OSS: {oss_url}")
                
                # ç”Ÿæˆè§†é¢‘åµŒå…¥ä»£ç ï¼Œå¹¶ä¿ç•™é‚®ä»¶æ­£æ–‡
                video_html = "\n\n".join([f'<video controls width="100%"><source src="{url}" type="video/mp4"></video>' for url in video_urls])
                if content:
                    # é‚®ä»¶æ­£æ–‡ + è§†é¢‘
                    content = f"{content}\n\n{video_html}"
                else:
                    content = video_html
            
            # ç¡®å®šå†…å®¹æ¥æº
            if content_type == ContentType.WEIXIN:
                content_source = 'weixin'
            elif content_type == ContentType.MEIPIAN:
                content_source = 'meipian'
            elif content_type == ContentType.OTHER_URL:
                content_source = 'other_url'
            elif content_type == ContentType.LARGE_ATTACHMENT:
                content_source = 'large_attachment'
            elif content_type == ContentType.ARCHIVE:
                # å‹ç¼©åŒ…
                content_source = 'archive'
            elif content_type == ContentType.VIDEO:
                content_source = 'video'
            elif doc_path:
                content_source = 'doc'
            elif docx_path:
                content_source = 'docx'
            else:
                content_source = 'text'
            
            # åˆ›å»ºæŠ•ç¨¿è®°å½•ï¼ˆä½¿ç”¨è§£æåçš„æ ‡é¢˜ï¼Œä¿å­˜åŸå§‹HTMLï¼‰
            submission = await submission_service.create_submission(
                email_subject=title or email_data.subject,
                email_from=email_data.from_addr,
                email_date=email_data.date,
                original_content=content,
                doc_file_path=doc_path,
                docx_file_path=docx_path
            )
            
            # æ›´æ–°content_sourceå’Œoriginal_html
            update_data = {'id': submission.id}
            if original_html:
                update_data['html'] = original_html
                logger.info(f"å·²ä¿å­˜åŸå§‹HTMLï¼Œé•¿åº¦: {len(original_html)}")
            
            await db.execute(
                text('UPDATE submissions SET original_html = :html, content_source = :source WHERE id = :id'),
                {'html': original_html, 'source': content_source, 'id': submission.id}
            )
            await db.commit()
            
            # ä¿å­˜è§£æçš„å…ƒæ•°æ®
            if cooperation or media_type or source_unit:
                site_id = await EmailParser.get_wordpress_site_id_async(media_type, db) if media_type else None
                
                # æ›´æ–°æŠ•ç¨¿è®°å½•çš„å…ƒæ•°æ®
                await db.execute(
                    text('''
                        UPDATE submissions 
                        SET cooperation_type = :cooperation,
                            media_type = :media,
                            source_unit = :source,
                            target_site_id = :site_id
                        WHERE id = :id
                    '''),
                    {
                        'cooperation': cooperation.value if cooperation else None,
                        'media': media_type.value if media_type else None,
                        'source': source_unit,
                        'site_id': site_id,
                        'id': submission.id
                    }
                )
                await db.commit()
                
                logger.info(f"å…ƒæ•°æ®å·²ä¿å­˜: åˆä½œ={cooperation}, åª’ä½“={media_type}, å•ä½={source_unit}, ç«™ç‚¹={site_id}")

            # å†™å…¥é™„ä»¶è®°å½•ï¼ˆWord/å‹ç¼©åŒ…/è§†é¢‘ç­‰ï¼‰
            if attachment_records:
                for rec in attachment_records:
                    try:
                        await submission_service.add_attachment(
                            submission_id=submission.id,
                            attachment_type=rec.get("attachment_type"),
                            oss_url=rec.get("oss_url"),
                            oss_key=rec.get("oss_key"),
                            original_filename=rec.get("original_filename"),
                            file_size=rec.get("file_size")
                        )
                    except Exception as e:
                        logger.error(f"é™„ä»¶è®°å½•å†™å…¥å¤±è´¥: {rec}, err={e}")
            
            # ä¸Šä¼ ä»ç½‘é¡µæŠ“å–çš„å›¾ç‰‡å¹¶æ›¿æ¢URL
            url_mapping = {}  # åŸå§‹URL -> OSS URLçš„æ˜ å°„ï¼ˆç”¨äº Markdown contentï¼‰
            oss_urls_ordered = []  # æŒ‰å›¾ç‰‡é¡ºåºçš„ OSS URL åˆ—è¡¨ï¼ˆç”¨äº HTML æŒ‰åºæ›¿æ¢ï¼‰
            original_image_urls = []  # ä¿å­˜åŸå§‹å›¾ç‰‡URLé¡ºåº
            
            for idx, (img_filename, img_data) in enumerate(images_to_upload):
                try:
                    oss_url, oss_key = oss_service.upload_file(
                        file_data=img_data,
                        filename=img_filename,
                        folder=f"submissions/{submission.id}"
                    )
                    
                    await submission_service.add_image(
                        submission_id=submission.id,
                        oss_url=oss_url,
                        oss_key=oss_key,
                        original_filename=img_filename,
                        file_size=len(img_data)
                    )
                    
                    oss_urls_ordered.append(oss_url)
                    
                    # è®°å½•åŸå§‹URLï¼ˆä»image_urlsåˆ—è¡¨è·å–ï¼‰
                    if idx < len(image_urls):
                        original_image_urls.append(image_urls[idx])
                    
                    # è®°å½•URLæ˜ å°„ï¼ˆç”¨äºæ›¿æ¢ Markdown contentï¼‰
                    if content_type in [ContentType.WEIXIN, ContentType.MEIPIAN]:
                        import re
                        pattern = rf'!\[å›¾ç‰‡{idx+1}\]\(([^\)]+)\)'
                        match = re.search(pattern, content)
                        if match:
                            original_url = match.group(1)
                            url_mapping[original_url] = oss_url
                            
                except Exception as e:
                    logger.error(f"ä¸Šä¼ å›¾ç‰‡å¤±è´¥: {str(e)}")
                    continue
            
            # æ›¿æ¢ Markdown content ä¸­çš„å›¾ç‰‡URLä¸º OSS URL
            if url_mapping:
                for original_url, oss_url in url_mapping.items():
                    content = content.replace(original_url, oss_url)
            
            # ç¾ç¯‡ï¼šä»HTMLç”Ÿæˆå›¾æ–‡æ··æ’çš„Markdown
            if content_type == ContentType.MEIPIAN and original_html and oss_urls_ordered:
                from bs4 import BeautifulSoup, NavigableString
                soup = BeautifulSoup(original_html, 'html.parser')
                content_tag = soup.find('div', {'class': 'mp-article-tpl'})
                
                if content_tag:
                    markdown_parts = []
                    img_index = 0
                    
                    # éå†æ‰€æœ‰å­å…ƒç´ ï¼Œä¿æŒå›¾æ–‡é¡ºåº
                    for elem in content_tag.descendants:
                        if elem.name == 'img' and img_index < len(oss_urls_ordered):
                            markdown_parts.append(f'\n![å›¾ç‰‡{img_index+1}]({oss_urls_ordered[img_index]})\n')
                            img_index += 1
                        elif isinstance(elem, NavigableString) and elem.strip():
                            elem_text = elem.strip()
                            if elem_text and not elem_text.startswith('[IMAGE_'):
                                markdown_parts.append(elem_text)
                    
                    content = '\n\n'.join([p for p in markdown_parts if p.strip()])
                    logger.info(f"ç¾ç¯‡Markdownå·²ç”Ÿæˆï¼Œä¿æŒå›¾æ–‡æ··æ’ï¼Œ{img_index}å¼ å›¾ç‰‡")
            
            # å…¬ä¼—å·/ç¾ç¯‡ï¼šæ›¿æ¢HTMLä¸­çš„å›¾ç‰‡URLä¸ºOSS URL
            if original_html and content_type in [ContentType.WEIXIN, ContentType.MEIPIAN] and len(oss_urls_ordered) > 0:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(original_html, 'html.parser')
                imgs = soup.find_all('img')
                for i, img in enumerate(imgs):
                    if i < len(oss_urls_ordered):
                        img['src'] = oss_urls_ordered[i]
                        if img.get('data-src'):
                            img['data-src'] = oss_urls_ordered[i]
                original_html = str(soup)
                logger.info(f"HTMLå·²æ›¿æ¢ {len(oss_urls_ordered)} ä¸ªå›¾ç‰‡URLä¸ºOSSåœ°å€")
            
            # æ›´æ–°æŠ•ç¨¿å†…å®¹å’Œ HTML
            if url_mapping or oss_urls_ordered:
                await db.execute(
                    text('UPDATE submissions SET original_content = :content, original_html = :html WHERE id = :id'),
                    {'content': content, 'html': original_html, 'id': submission.id}
                )
                await db.commit()
                logger.info(f"å·²æ›¿æ¢ {len(oss_urls_ordered) or len(url_mapping)} ä¸ªå›¾ç‰‡URLä¸ºOSSåœ°å€")
            
            # Wordåˆ†æ”¯ï¼šå›¾ç‰‡æå–å®Œæˆåå†æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if content_type == ContentType.WORD:
                if doc_path and os.path.exists(doc_path):
                    os.unlink(doc_path)
                    logger.info(f"å·²æ¸…ç†Wordä¸´æ—¶æºæ–‡ä»¶: {doc_path}")
                if docx_path and os.path.exists(docx_path) and docx_path != doc_path:
                    os.unlink(docx_path)
                    logger.info(f"å·²æ¸…ç†Wordä¸´æ—¶docxæ–‡ä»¶: {docx_path}")
            
            # åˆ›å»ºåŸæ–‡è‰ç¨¿ï¼ˆä¾›ç¼–è¾‘äººå‘˜æŸ¥çœ‹å’Œæ‰‹åŠ¨ç¼–è¾‘ï¼‰
            from app.services.draft_service import DraftService
            from app.utils.content_processor import ContentProcessor
            draft_service = DraftService(db)
            
            # å…¬ä¼—å·ã€ç¾ç¯‡ã€OTHER_URLï¼ˆæœ‰åŸå§‹HTMLæ—¶ï¼‰ï¼šå°†HTMLè½¬æ¢ä¸ºMarkdownï¼Œå¹¶æ’å…¥å ä½ç¬¦
            if (content_type in [ContentType.WEIXIN, ContentType.MEIPIAN, ContentType.OTHER_URL] and original_html and oss_urls_ordered):
                import html2text
                from bs4 import BeautifulSoup
                
                soup = BeautifulSoup(original_html, 'html.parser')
                img_tags = soup.find_all('img')
                # åªæ›¿æ¢å‰ N å¼ ï¼ˆä¸ oss é¡ºåºä¸€è‡´ï¼‰ï¼Œé¿å…å ä½ç¬¦ä¸ media_map é”™ä½
                for idx, img in enumerate(img_tags, start=1):
                    if idx <= len(oss_urls_ordered):
                        placeholder = f'[[IMG_{idx}]]'
                        img.replace_with(placeholder)
                
                h = html2text.HTML2Text()
                h.ignore_links = False
                h.ignore_images = False
                h.body_width = 0
                draft_content = h.handle(str(soup))
                if content_type == ContentType.OTHER_URL:
                    logger.info(f"OTHER_URL å·²ä» HTML ç”Ÿæˆå¸¦å ä½ç¬¦çš„è‰ç¨¿ï¼Œå›¾ç‰‡æ•°: {len(oss_urls_ordered)}")
            else:
                draft_content = content

            # Word/å‹ç¼©åŒ…/OTHER_URLï¼ˆæœ‰å›¾æ—¶ï¼‰ï¼šæ˜¾å¼æ„å»º media_mapï¼Œé¿å…å ä½ç¬¦ä¸å›¾ç‰‡é”™ä½æˆ–ä¸¢å¤±
            if content_type in [ContentType.WORD, ContentType.ARCHIVE, ContentType.OTHER_URL] and oss_urls_ordered:
                draft_media_map = {
                    f"[[IMG_{idx}]]": oss_url
                    for idx, oss_url in enumerate(oss_urls_ordered, start=1)
                }
                draft = await draft_service.create_draft(
                    submission_id=submission.id,
                    original_content_md=draft_content,
                    ai_content_md=draft_content,
                    media_map=draft_media_map
                )
            else:
                draft = await draft_service.create_draft(
                    submission_id=submission.id,
                    transformed_content=draft_content
                )
            logger.info(f"å·²åˆ›å»ºåŸæ–‡è‰ç¨¿: draft_id={draft.id}, content_type={content_type}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºcompleted
            await submission_service.update_status(submission.id, 'completed')
            
            # è‹¥ä¸ºæ›¿æ¢ç¨¿ï¼Œè®°å½•è¢«æ›¿æ¢çš„æ—§ç¨¿åˆ° duplicate_logs
            if superseded_id:
                dup_log = DuplicateLog(
                    email_subject=email_data.subject,
                    email_from=email_data.from_addr,
                    email_date=email_data.date,
                    cooperation_type=cooperation.value if cooperation else None,
                    media_type=media_type.value if media_type else None,
                    source_unit=source_unit,
                    title=title,
                    duplicate_type="superseded",
                    effective_submission_id=submission.id,
                    superseded_submission_id=superseded_id,
                )
                db.add(dup_log)
                await db.commit()
                logger.info(f"å·²è®°å½•æ›¿æ¢å…³ç³»: æ—§ç¨¿ID={superseded_id} -> æ–°ç¨¿ID={submission.id}")
            
            # è®°å½•ä»»åŠ¡æˆåŠŸ
            await submission_service.log_task(
                task_type="fetch_email",
                task_id=str(submission.id),
                status="success",
                message=f"é‚®ä»¶å¤„ç†æˆåŠŸ: submission_id={submission.id}"
            )
            
            logger.info(f"é‚®ä»¶å¤„ç†æˆåŠŸ: submission_id={submission.id}, ç­‰å¾…ç¼–è¾‘äººå‘˜æ“ä½œ")
        
        except Exception as e:
            error_msg = f"é‚®ä»¶å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            # è®°å½•ä»»åŠ¡å¤±è´¥
            await submission_service.log_task(
                task_type="fetch_email",
                task_id=None,
                status="failed",
                message=error_msg
            )
            
            raise


@celery_app.task(name="convert_doc_to_docx")
def convert_doc_to_docx_task(doc_path: str) -> str:
    """
    ä½¿ç”¨LibreOfficeè½¬æ¢æ–‡æ¡£æ ¼å¼
    
    Args:
        doc_path: .docæ–‡ä»¶è·¯å¾„
    
    Returns:
        str: è½¬æ¢åçš„.docxæ–‡ä»¶è·¯å¾„
    """
    try:
        processor = DocumentProcessor()
        docx_path = processor.convert_doc_to_docx(doc_path)
        logger.info(f"æ–‡æ¡£è½¬æ¢æˆåŠŸ: {docx_path}")
        return docx_path
    except Exception as e:
        logger.error(f"æ–‡æ¡£è½¬æ¢å¤±è´¥: {str(e)}")
        raise


@celery_app.task(name="extract_images_from_docx")
def extract_images_task(docx_path: str) -> list:
    """
    ä»docxæå–å›¾ç‰‡å¹¶ä¸Šä¼ OSS
    
    Args:
        docx_path: .docxæ–‡ä»¶è·¯å¾„
    
    Returns:
        list: ä¸Šä¼ åçš„å›¾ç‰‡URLåˆ—è¡¨
    """
    try:
        processor = DocumentProcessor()
        oss_service = OSSService()
        
        # æå–å›¾ç‰‡
        images = processor.extract_images_from_docx(docx_path)
        
        # ä¸Šä¼ åˆ°OSS
        uploaded_urls = []
        for filename, image_data in images:
            oss_url, oss_key = oss_service.upload_file(
                file_data=image_data,
                filename=filename
            )
            uploaded_urls.append(oss_url)
        
        logger.info(f"æˆåŠŸä¸Šä¼  {len(uploaded_urls)} å¼ å›¾ç‰‡")
        return uploaded_urls
    
    except Exception as e:
        logger.error(f"å›¾ç‰‡æå–ä¸Šä¼ å¤±è´¥: {str(e)}")
        raise
